# -*- mode: qore; indent-tabs-mode: nil -*-
#! @file Mapper.qm data mapping module

/*  Mapper.qm Copyright 2014 - 2020 Qore Technologies, s.r.o.

    Permission is hereby granted, free of charge, to any person obtaining a
    copy of this software and associated documentation files (the "Software"),
    to deal in the Software without restriction, including without limitation
    the rights to use, copy, modify, merge, publish, distribute, sublicense,
    and/or sell copies of the Software, and to permit persons to whom the
    Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in
    all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.
*/

# minimum required Qore version
%requires qore >= 0.9.4

# require type definitions everywhere
%require-types

# enable all warnings
%enable-all-warnings

# do not use "$" for vars
%new-style

# common definitions
%requires(reexport) MapperUtil

# supports using the DataProvider module to describe input and output records
%requires(reexport) DataProvider

module Mapper {
    version = "1.5.3";
    desc = "user module providing basic data mapping infrastructure";
    author = "David Nichols <david@qore.org>";
    url = "http://qore.org";
    license = "MIT";
}

/** @mainpage Mapper Module

    @tableofcontents

    @section mapperintro Mapper Module Introduction

    This module provides classes that help with structured data mapping, meaning the transformation of data in one or more input
    formats to a different output format.

    Classes provided by this module:
    - @ref Mapper::Mapper "Mapper": the base data mapping class
    - @ref Mapper::AbstractMapperIterator "AbstractMapperIterator": an abstract base class for iterator mapper classes
    - @ref Mapper::MapperIterator "MapperIterator": a class that automatically applies a data mapper to iterated data

    @section mapperexamples Mapper Examples

    The following is an example map hash with comments:
    @code{.py}
const DataMap = (
    # output field: "id" mapper from the "Id" element of any "^attributes^" hash in the input record
    "id": "^attributes^.Id",
    # output field: "name": maps from an input field with the same name (no translations are made)
    "name": True,
    # output field: "explicit_count": maps from the input "Count" field, if any value is present then it is converted to an integer
    "explicit_count": ("type": "int", "name": "Count"),
    # output field: "implicit_count": runs the given code on the input record and retuns the result, the code returns the number of "Products" sub-records
    "implicit_count": int sub (any ignored, hash rec) { return rec.Products.size(); },
    # output field: "order_date": converts the "OrderDate" string input field to a date in the specified format
    "order_date": ("name": "OrderDate", "date_format": "DD.MM.YYYY HH:mm:SS.us"),
);
    @endcode

    If this map is applied to the following data in the following way:
    @code{.py}
const MapInput = ((
    "^attributes^": ("Id": 1),
    "name": "John Smith",
    "Count": 1,
    "OrderDate": "02.01.2014 10:37:45.103948",
    "Products": ((
        "ProductName": "Widget 1",
        "Quantity": 1,
        ),
    )), (
    "^attributes^": ("Id": 2),
    "name": "Steve Austin",
    "Count": 2,
    "OrderDate": "04.01.2014 19:21:08.882634",
    "Products": ((
        "ProductName": "Widget X",
        "Quantity": 4,
        ), (
        "ProductName": "Widget 2",
        "Quantity": 2,
        ),
    )),
);

Mapper mapv(DataMap);
list l = mapv.mapAll(MapInput);
printf("%N\n", l);
    @endcode

    The result will be:
    @verbatim
list: (2 elements)
  [0]=hash: (5 members)
    id : 1
    name : "John Smith"
    explicit_count : 1
    implicit_count : 1
    order_date : 2014-01-02 10:37:45.103948 Thu +01:00 (CET)
  [1]=hash: (5 members)
    id : 2
    name : "Steve Austin"
    explicit_count : 2
    implicit_count : 2
    order_date : 2014-01-04 19:21:08.882634 Sat +01:00 (CET))
    @endverbatim

    @section mapperkeys Mapper Specification Format

    The mapper hash is made up of target (ie output) field names (note that dotted output field names result in a nested hash output unless the \a allow_output_dot option is set) as the key values assigned to field specifications as follows:
    - @ref True "True": this is a shortcut meaning map from an input field with the same name
    - a @ref string_type "string": giving the input field name directly (equivalent to a hash with the \c "name" key)
    - a @ref closure "closure" or @ref call_reference "call reference": meaning map from a field of the same name an apply the given code to give the value for the mapping (equivalent to a hash with the \c "code" key); the @ref closure "closure" or @ref call_reference "call reference" must accept the following arguments:
      - @ref any_type "any" <i>value</i>: the input field value (with the same name as the output field; to use a different name, see the \a code hash option below)
      - @ref hash_type "hash" <i>rec</i>: the current input record
    - a @ref hash_type "hash" describing the mapping; the following keys are all optional (an empty hash means map from an input field with the same name with no translations):
      - \c "code": a closure or call reference to process the field data; cannot be used with the \c "constant" or \c "index" keys
      - \c "constant": the value of this key will be returned as a constant value; this key cannot be used with the \c "name", \c "struct", \c "code", \c "index" or \c "default" keys
      - \c "index": gives current index/count of the row. The initial int value is the start offset. So value 0 means that mapped values will be: 0, 1, ..., N; 1 means: 1, 2, ..., N; etc.
      - \c "date_format": gives the format for converting an input string to a date; see @ref date_formatting for the format of this string; note that this also implies \c "type" = \c "date"
      - \c "default": gives a default value for the field in case no input or translated value is provided
      - \c "mand": assign to boolean @ref True "True" if the field is mandatory and an exception should be thrown if no input data is supplied
      - \c "maxlen": an integer giving the maximum output string field length in bytes
      - \c "name": the value of this key gives the name of the input field; only use this if the input record name is different than the output field name; note that if this value contains \c "." characters and the \a allow_dot option is not set (see @ref mapperoptions), then the value will be treated like \c "struct" (the \c "struct" key value will be created automatically); cannot be used with the \c "constant" ior \c "index" keys
      - \c "number_format": gives the format for converting an input string to a number; see @ref Qore::parse_number() for the format of this string; note that this also implies \c "type" = \c "number"
      - \c "output_key_path": gives the output path for hash output values; each element in the list is a string key name
      - \c "runtime": a reference to @ref mapper_runtime_handling current status. The value is key in the current runtime structure.
      - \c "struct": the value of this key gives the location of the input field in an input hash in dot notation, ex: \c "element.name" would look for the field's value in the \c "name" key of the \c "element" hash in the input record; cannot be used with the \c "constant" or \c "index" keys; this option is only necessary in place of the "name" option if the \a allow_dot option is set, otherwise use \c "name" instead
      - \c "trunc": assign to boolean @ref True "True" if the field should be truncated if over the maximum field length; this key can only be set to @ref True "True" if the \c "maxlen" key is also given
      - \c "type": this gives the output field type, can be:
        - \c "date": date/time field
        - \c "int": fields accepts only integer values (any non-integer values on input will cause an exception to be thrown when mapping; note: also \c "integer" is accepted as an alias for \c "int")
        - \c "number": field accepts only numeric values (any non-numeric values on input will cause an exception to be thrown when mapping); numeric values are left in their original types, any other type is converted to a @ref number_type "arbitrary-precision numeric" value
        - \c "string": field accepts string values; in this case any other value will be converted to a string in the output
        - \c "hash": field accepts hash values
        - \c "any": field accepts any value
      - \c "type_options": a hash of type options to set or override type options for the output field

    @section mapperoptions Mapper Options

    Mapper objects accept the following options in the option hash:
    - \c "info_log": an optional info logging callback; must accept a string format specifier and sprintf()-style
      arguments
    - \c "input": this should be a description of the input fields with type
      <tt>hash&lt;string, DataProvider::AbstractDataField&gt;</tt>, for backwards compatibility, this option also
      accepts a hash describing the input fields where each key is a possible input field name (and where dot notation
      indicates a multi-level hash) and each value is a hash describing the field with the following optional keys:
      - \c "desc": this gives the description of the input field
      .
      This option is mutually exclusive with the \a input_provider option
    - \c "input_log": an optional input data logging callback; must accept a hash giving the input data hash
    - \c "input_provider": gives the input provider with an
      @ref DataProvider::AbstractDataProvider "AbstractDataProvider" object which defines the type of input data and
      also the data itself.  The use of this option enables the use of the
      @ref Mapper::Mapper::getOutputIterator() "Mapper::getOutputIterator()" API.  This option is mutually exclusive
      with the \a input option.  If an \c "output_provider" is also provided, the
      @ref Mapper::Mapper::runAutonomous() "Mapper::runAutonomous()" method can be used to map from input to output
      in a single call
    - \c "input_provider_search": the search criteria for the input provider; see the \a where_cond option of
      @ref DataProvider::AbstractDataProvider::searchRecords() "AbstractDataProvider::searchRecords()" for more
      information on this option
    - \c "input_request": the arguments for input providers using the request/response API
    - \c "input_request_options": any options to input providers using the request/response API
    - \c "input_response_error": a string indicating the input providers using the request/response API should use
      the given error response message for the record format
    - \c "input_search_options": the search options for the input provider; see the \a search_options option of
      @ref DataProvider::AbstractDataProvider::searchRecords() "AbstractDataProvider::searchRecords()" for more
      information on this option
    - \c "name": the name of the mapper for use in logging and error strings
    - \c "output": this should be a description of the output fields with type
      <tt>hash&lt;string, DataProvider::AbstractDataField&gt;</tt>, for backwards compatibility, this option also
      accepts a hash describing the output data structure where each hash key is a output field name (and where dot
      notation indicates a multi-level hash) and each value is an optional hash describing the output field taking a
      subset of @ref mapperkeys "mapper field hash keys" as follows:
      - \c "desc": a description of the output field
      - \c "mand": @ref True "True" if the field is mandatory and an exception should be thrown if no input data
        is supplied
      - \c "maxlen": an integer giving the maximum length of a string field in bytes
      - \c "type": this gives the output field type, can be:
        - \c "date": date/time field
        - \c "int": fields accepts only integer values (any non-integer values on input will cause an exception to be
          thrown when mapping; note: also \c "integer" is accepted as an alias for \c "int")
        - \c "number": field accepts only numeric values (any non-numeric values on input will cause an exception to
          be thrown when mapping); numeric values are left in their original types, any other type is converted to a
          @ref number_type "arbitrary-precision numeric" value
        - \c "string": field accepts string values; in this case any other value will be converted to a string in the
          output
        - \c "hash": field accepts hash values
        - \c "any": field accepts any value
    - \c "output_log": an optional output data logging callback; must accept a hash giving the output data hash
    - \c "output_nullable": set all output fields as nullable
    - \c "output_provider": gives the output provider with an
      @ref DataProvider::AbstractDataProvider "AbstractDataProvider" object which defines the type of output data and
      also location where the output data will be written.  If this option is set, then every mapped record will be
      written to the output data provider automatically.
      - record-based output providers: The mapped output is written to the output provider as records.  If the output
        provider supports transaction management. @ref Mapper::Mapper::commit() "Mapper::commit()" and
        @ref Mapper::Mapper::rollback() "Mapper::rollback()" can be used.
      - request/reply output providers: The mapped output is used as the request data for the output provider and above
        request is made for each output record.
      .
      This option is mutually exclusive with the \a output option.  If an \c "input_provider" is also provided, the
      @ref Mapper::Mapper::runAutonomous() "Mapper::runAutonomous()" method can be used to map from input to output
      in a single call
    - \c "output_provider_bulk": if this option is used with a record-based output provider, then bulk operations are
      used with the output provider, and the @ref Mapper::Mapper::flushOutput() "Mapper::flushOutput()" method must be
      called after all mapping is done to flush the output buffer to the output provider at the end, or
      @ref Mapper::Mapper::discardOutput() "Mapper::discardOutput()" must be called to discard any data left in the
      bulk output buffer if the results should be discarded (ex: the output provide requires transaction management
      and an error occurs causing the transaction to be rolled back)
    - \c "output_provider_upsert": set to @ref True if upsert operations instead of creation APIs should be used with
      the output provider.  If \a output_provider_bulk is also set, this indicates if the
      @ref DataProvider::AbstractDataProviderBulkOperation "AbstractDataProviderBulkOperation" object will use upsert
      operations instead of insert operations
    - \c "runtime": an initial runtime structure for @ref mapper_runtime_handling
    - \c "trunc_all": if @ref True "True" (as evaluated by @ref Qore::parse_boolean() "parse_boolean()") then
      any field without a \c "trunc" key (see @ref mapperkeys \c "trunc" description) will automatically be truncated
      if a \c "maxlen" attribute is set for the field

    The following deprecated options are also accepted:
    - \c "allow_dot": if @ref True "True" (as evaluated by @ref Qore::parse_boolean() "parse_boolean()") then
      field names with \c "." characters do not imply a structured internal element lookup; in this case input field
      names may have \c "." characters in them, use the \c "struct" key to use structured internal element loopups
      (see @ref mapperkeys \c "struct" docs for more info)
    - \c "allow_output_dot": if @ref True "True" (as evaluated by @ref Qore::parse_boolean() "parse_boolean()")
      then output field names with \c "." characters do not imply a structured/hash output element; in this case
      output field names may have \c "." characters in them
    - \c "date_format": gives the global format for converting a string to a date; see @ref date_formatting for the
      format of this string; this is applied to all fields of type \c "date" unless the field has a \c "date_format"
      value that overrides this global setting
    - \c "encoding": the output character encoding; if not present then \c "UTF-8" is assumed
    - \c "input_timezone": an optional string or integer (giving seconds east of UTC) giving the time zone for parsing
      input data (ex: \c "Europe/Prague"), if not set defaults to the current TimeZone (see @ref Qore::TimeZone::get())
    - \c "number_format": gives the global format for converting a string to a number; see @ref Qore::parse_number()
      for the format of this string; this is applied to all fields of type \c "number" unless the field has a
      \c "number_format" value that overrides this global setting
    - \c "timezone": an optional string or integer (giving seconds east of UTC) giving the time zone definition for
      output data (ex: \c "Europe/Prague"), if not set defaults to the current TimeZone (see @ref Qore::TimeZone::get())

    @note
    - if the \c "input" option is given, then only those defined fields can be referenced as input fields in the
      @ref mapperkeys "mapper hash"; all possible input fields should be defined here if this option is used
    - if the \c "output" option is given, then only those defined fields can be referenced as output fields,
      additionally the types given in the output definition cannot be overridden in the @ref mapperkeys "mapper hash";
      all possible output fields should be defined here if this option is used

    @section mapper_runtime_handling Mapper Runtime Options

    Runtime options for @ref Mapper::Mapper "Mapper" objects allow the programmer to use constant values provided at runtime
    in the @ref Mapper::Mapper "Mapper" output.

    For example, runtime options can be useful in the following cases:
        - storing one date/time value for all output hashes of the @ref Mapper::Mapper "Mapper"
        - using a value from a database sequence value for the lifetime of the @ref Mapper::Mapper "Mapper" object

    @par Example:
    @code{.py}
hash<auto> mapv = (
    "foo": ("constant": "bar"),
     # ...
    "date_begin": ("runtime": "start_date"), # references runtime option "start_date"
    "group": ("runtime": "group_id"),        # references runtime option "group_id"
);
hash<auto> opts = (
    "timezone": "Europe/Prague",
    # ...
    "runtime": (
        "start_date": now_us(), # set runtime option "start_date"
        "group_id": 0,          # set runtime option "group_id" to 0
    ),
);
Mapper m(mapv, opts);        # runtime options are active now
m.mapData(input1);           # output record hash date_begin = start_date = timestamp of the opts creation and group = 0
    @endcode

    The runtime options are basically the same as setting constants in the mapper before providing runtime data to the mapper. As such, the runtime options can be changed only before the first input hash is processed by a @ref Mapper::Mapper "Mapper".

    Note that the @ref Mapper::Mapper::setRuntime() "Mapper::setRuntime()" and @ref Mapper::Mapper::replaceRuntime() "Mapper::replaceRuntime()" methods are deprecated - please use @ref Mapper::Mapper "Mapper" construction options to set runtime values instead. The methods are deprecated since runtime options duplicate existing functionality and are confusing and error-prone to use.

    @section mapperrelnotes Release Notes

    @subsection mapperv1_5_3 Mapper v1.5.3
    - fixed a bug handling external runtime keys with bulk input for keys that do not require the current input value
      (<a href="https://github.com/qorelanguage/qore/issues/3931">issue 3931</a>)

    @subsection mapperv1_5_2 Mapper v1.5.2
    - added the @ref Mapper::Mapper::mapAutoInput() "Mapper::mapAutoInput()" method
    - added the following \c output_create_ignore_duplicates option
    - fixed a bug where mapper output data was not logged in case of an error in an output provider
      (<a href="https://github.com/qorelanguage/qore/issues/3909">issue 3909</a>)
    - added support for mapper context in mapper field key handlers
      (<a href="https://github.com/qorelanguage/qore/issues/3893">issue 3893</a>)
    - fixed @ref Mapper::Mapper::mapAuto() "Mapper::mapAuto()" to return @ref NOTHING with no input
      (<a href="https://github.com/qorelanguage/qore/issues/3872">issue 3872</a>)
    - implemented support for nested mappers including the \c submappers option
      (<a href="https://github.com/qorelanguage/qore/issues/3414">issue 3414</a>)

    @subsection mapperv1_5_1 Mapper v1.5.1
    - added the \c ignore_missing_input mapper option
      (<a href="https://github.com/qorelanguage/qore/issues/3837">issue 3837</a>)
    - added support for the \c use_input_record output field mapping key
      (<a href="https://github.com/qorelanguage/qore/issues/3823">issue 3823</a>)
    - added the @ref Mapper::Mapper::setNullableOutput() "Mapper::setNullableOutput()" method and the
      \c output_nullable option
      (<a href="https://github.com/qorelanguage/qore/issues/3788">issue 3788</a>)

    @subsection mapperv1_5 Mapper v1.5
    - added support for the DataProvider module to describe input and output records
      (<a href="https://github.com/qorelanguage/qore/issues/3545">issue 3545</a>)

    @subsection mapperv1_4_1 Mapper v1.4.1
    - fixed a bug where list values could not be passed as a value in non-bulk mode
      (<a href="https://github.com/qorelanguage/qore/issues/3611">issue 3611</a>)
    - added support for types "any" and "hash"
      (<a href="https://github.com/qorelanguage/qore/issues/3453">issue 3453</a>)
    - added support for dot notation in output fields for the "hash" output type
      (<a href="https://github.com/qorelanguage/qore/issues/3413">issue 3413</a>)

    @subsection mapperv1_4 Mapper v1.4
    - added support for complex types
    - fixed a bug in the \c STRING-TOO-LONG exception (<a href="https://github.com/qorelanguage/qore/issues/2495">issue 2405</a>)

    @subsection mapperv1_3_1 Mapper v1.3.1
    - fixed bugs handling mapper fields with no input records in list mode as passed from the \c TableMapper module (<a href="https://github.com/qorelanguage/qore/issues/1736">issue 1736</a>)

    @subsection mapperv1_3 Mapper v1.3
    - internal updates to allow for TableMapper insert performance improvements (<a href="https://github.com/qorelanguage/qore/issues/1626">issue 1626</a>)

    @subsection mapperv1_2 Mapper v1.2
    - significantly improved mapper performance with identity (i.e. 1:1) and constant mappings (<a href="https://github.com/qorelanguage/qore/issues/1620">issue 1620</a>)

    @subsection mapperv1_1 Mapper v1.1
    - implemented \c "constant" field tag giving a constant value for the output of a field
    - implemented structured output for dotted output field names and the \c "allow_output_dot" option to suppress this behavior
    - implemented \c "default" field tag giving a default value if no input value is specified
    - moved field length checks after all transformations have been applied
    - implemented a global \c "date_format" mapper option
    - implemented the \c "number_format" field option and a global option of the same name
    - fixed bugs in the \c "timezone" and \c "input_timezone" options, documented those options
    - changed the behavior of the \c "number" field type: now leaves numeric values in their original type, converts all other types to a number
    - removed the deprecated \c "crec" option
    - implemented the \c "input" option with input record validation
    - implemented the \c "output" option with output record validation
    - implemented the \c "info_log" option and removed the \c "trunc" option
    - added runtime option handling (@ref mapper_runtime_handling):
      - \c "runtime" mapper option
      - @ref Mapper::Mapper::getRuntime()
      - @ref Mapper::Mapper::replaceRuntime()
      - @ref Mapper::Mapper::setRuntime()
    - implemented \c "index" field tag for current row index
    - improved the @ref Mapper::Mapper::mapAll() method by adding support for hashes of lists to better support input from bulk DML (@ref Qore::SQL::SQLStatement::fetchColumns() "SQLStatement::fetchColumns()")

    @subsection mapperv1_0 Mapper v1.0
    - Initial release
*/

#! the Mapper namespace contains all the definitions in the Mapper module
public namespace Mapper {
#! describes type options
public hashdecl MapperOptionInfo {
    #! the option value type
    string type;

    #! the description of the option
    string desc;
}

#! this class is a base class for mapping data; see @ref mapperexamples for usage examples
public class Mapper inherits AbstractDataProcessor {
    public {
        #! field keys that conflict with "constant" and "index"
        const ConstantConflictList = ("name", "struct", "code", "default");

        #! constructor option keys (can be extended by subclassing and reimplementing optionKeys())
        const OptionKeys = (map {$1.key: $1.value.desc}, UserOptions.pairIterator()) + {
            "allow_dot": "DEPRECATED: allows input fields to have a dot in their name without implying a "
                "structured format",
            "allow_output_dot": "DEPRECATED: allows output fields to have a dot in their name without implying a "
                "structured format",
            "date_format": "DEPRECATED: gives the default format for parsing dates from strings; ex: "
                "\"MM/DD/YYYY HH:mm:SS\"",
            "empty_strings_to_nothing": "DEPRECATED: converts output record's empty strings to NOTHING",
            "encoding": "DEPRECATED: gives the default output character encoding for string fields",
            "info_log": "a call reference / closure for informational logging",
            "input": "the input record description (mutually exclusive with input_provider)",
            "input_log": "a call reference / closure for input record logging",
            "input_provider": "the input provider (mutually exclusive with input)",
            "input_timezone": "DEPRECATED: the default timezone to assume when parsing input dates",
            "mapper_thread_context": "an optional thread context hash",
            "name": "the name of the Mapper object",
            "number_format": "DEPRECATED: the default number format when parsing number fields from strings; ex: "
                "\".,\"",
            "output": "the output record description (mutually exclusive with output_provider)",
            "output_log": "a call reference / closure for input record logging",
            "output_provider": "the output data provider (mutually exclusive with output)",
            "runtime_keys": "field key support provided as a Mapper option; format: "
                "hash<string, hash<MapperRuntimeKeyInfo>>",
            "submapper_get": "a call reference / closure for retrieving submappers "
                "(sig: sub (string name, *hash<auto> opts) {})",
            "template_subst": "a call reference / closure for template substitution; used and required for the "
                "\"submapper_options\" mapper key",
            "timezone": "DEPRECATED: the default output timezone for date/time values",
        };

        #! User options
        const UserOptions = {
            "disable_bulk": <MapperOptionInfo>{
                "type": "bool",
                "desc": "a flag to disable bulk operations when there is a hash of lists as input when calling "
                    "mapAuto",
            },

            "global_type_options": <MapperOptionInfo>{
                "type": "type-option-hash",
                "desc": "a hash of global type options to be used by applicable fields if they do not override the "
                    "option",
            },

            "ignore_missing_input": <MapperOptionInfo>{
                "type": "bool",
                "desc": "if true and if an output field has an input mapping but the input hash has no such value, "
                    "and the output field does not require a value, in this case, the output field is omitted "
                    "entirely; this option is useful when the mapper output is used for delta updates with variable "
                    "input, for example; this option cannot be used with bulk input",
            },

            "input_provider_search": <MapperOptionInfo>{
                "type": "hash",
                "desc": "the input provider search criteria",
            },

            "input_request": <MapperOptionInfo>{
                "type": "any",
                "desc": "the arguments for input providers using the request/response API",
            },

            "input_request_options": <MapperOptionInfo>{
                "type": "hash",
                "desc": "any options to input providers using the request/response API",
            },

            "input_response_error": <MapperOptionInfo>{
                "type": "string",
                "desc": "a string indicating the input providers using the request/response API should "
                "use the given error response message for the record format",
            },

            "input_search_options": <MapperOptionInfo>{
                "type": "hash",
                "desc": "the input provider search options",
            },

            "mapper_handler_context": <MapperOptionInfo>{
                "type": "hash",
                "desc": "a hash that will be passed to any field handlers as the context argument",
            },

            "output_nullable": <MapperOptionInfo>{
                "type": "bool",
                "desc": "set output fields as nullable",
            },

            "output_provider_bulk": <MapperOptionInfo>{
                "type": "bool",
                "desc": "a flag to enable bulk operations with an output provider; this option only has an "
                    "effect when used with an output provider that supports bulk operations on output",
            },

            "output_provider_upsert": <MapperOptionInfo>{
                "type": "bool",
                "desc": "a flag to enable upserting instead of creating records with an output "
                "data provider; this option only has an effect when used with an output provider",
            },

            "output_request_options": <MapperOptionInfo>{
                "type": "hash",
                "desc": "any options to output providers using the request/response API",
            },

            "output_create_ignore_duplicates": <MapperOptionInfo>{
                "type": "bool",
                "desc": "a flag that indicates that duplicate records should be ignored when creating records with "
                    "an output data provider",
            },

            "runtime": <MapperOptionInfo>{
                "type": "hash",
                "desc": "defines key-value pairs lookups for the \"runtime\" field mapping key",
            },

            "submappers": <MapperOptionInfo>{
                "type": "hash",
                "desc": "each hash key is the name of a global submapper and must have a hash value giving the "
                    "options for submapper as a value; requires the \"submapper_get\" option as well",
            },
        };

        #! default known mapper hash field keys (can be extended by subclassing and reimplementing validKeys())
        const ValidKeys = (map {$1: True}, keys MapperKeyInfo) + {
            # internal structures
            "struct": True,
            "output_key_path": True,

            # DEPRECATED keys
            "maxlen": True,                   # should be provided in the output record field
            "mand": True,                     # should be provided in the output record field type
            "number": True,                   # should be provided in the output record field
            "type": True,                     # type must be specified in the output record
            "date_format": True,              # should be set as a transformation option
            "number_format": True,            # should be set as a transformation option
            "empty_strings_to_nothing": True, # should be set as a transformation option
        };

        #! output option keys
        const OutputKeys = {
            "desc": True,
            "mand": True,
            "maxlen": True,
            "type": True,
        };

        #! the input and output record type
        static Type recordType("hash<string, AbstractDataField>");
    }

    private {
        #! the hash providing output field names and mappings
        /** note that we must strip types here
        */
        hash mapc;

        #! the hash with a subset of the mappings used dynamically
        hash<auto> mapd;

        #! the hash of output records for key order
        hash<auto> mapo;

        #! the optional name for the object (for example a table name); will be prepended to field names in error messages
        *string name;

        #! an optional info logging callback; must accept a sprintf()-style format specifier and optional arguments
        *code info_log;

        #! an optional input data logging callback; must accept a hash giving the input data hash
        *code input_log;

        #! an optional output data logging callback; must accept a hash giving the output data hash
        *code output_log;

        #! an optional callback for retrieving a nested mapper; sig: <tt>sub (string name, *hash<auto> opts) {}</tt>
        *code submapper_get;

        #! an optional callback for substituting templated values
        *code template_subst;

        #! global transformation options; can be overridden on a per-field basis
        hash<auto> global_transform_opts = {} + {
            "date.input_timezone": TimeZone::get(),
        };

        #! truncate all strings quietly / automatically to their maximum length
        bool trunc_all = False;

        #! DEPRECATED: do not assume \a struct when field names have a \c "." in them; instead allow input field names to have a \c "." in them
        /** only used when a plain hash is provided for input

            @deprecated use a field description for input instead
        */
        bool allow_dot = False;

        #! DEPRECATED: do not assume structured/hash output when output field names have a \c "." in them; instead allow output field names to have a \c "." in them
        /** only used when a plain hash is provided for output

            @deprecated use a field description for output instead
        */
        bool allow_output_dot = False;

        #! an optional description of possible input hash keys
        *hash<string, AbstractDataField> input;

        #! the optional input data provider
        *AbstractDataProvider input_provider;

        #! search conditions for the input provider
        *hash<auto> input_provider_search;

        #! search options for the input provider
        *hash<auto> input_search_options;

        #! indicates that the request/response API should be used for mapper input
        bool input_do_request;

        #! the argument for the request/response API for the input provider
        auto input_request;

        #! options for the request when using the request/response API for the input provider
        *hash<auto> input_request_options;

        #! a boolean flag that indicates that an input provider that supports both the record API and the request/response API should use the request search API to provide records
        bool input_request_search;

        #! supresses the output field if the input field is missing and the output is optional
        bool ignore_missing_input;

        #! disable bulk operations when there is a hash of lists as input when calling mapAuto
        bool disable_bulk;

        #! an optional description of the output data structure
        *hash<string, AbstractDataField> output;

        #! the optional output data provider
        *AbstractDataProvider output_provider;

        #! bulk output object for an output data provider
        *AbstractDataProviderBulkOperation output_provider_bulk_operation;

        #! if the upsert operations should be used on the output provider
        bool output_provider_upsert;

        #! indicates that the request/response API should be used for mapper output
        bool output_do_request;

        #! options for the request when using the request/response API for the output provider
        *hash<auto> output_request_options;

        #! set output fields nullable
        bool output_nullable = False;

        #! flag if the field descriptions were provided to the constructor
        /** if so then allow_dot cannot be set
        */
        bool structured_input;

        #! flag if the field descriptions were provided to the constructor
        /** if so then allow_output_dot cannot be set
        */
        bool structured_output;

        #! flag indicating that duplicate records should be ignored when creating records with an output mapper
        bool output_create_ignore_duplicates;

        #! count of records mapped
        int count = 0;

        #! current runtime values
        /** @since Mapper 1.1
        */
        *hash<auto> m_runtime;

        #! map of fields to be mapped 1:1 input -> output
        hash<auto> identh;

        #! list of fields to be mapped 1:1 input -> output
        *list<auto> identl;

        #! map of constant fields
        hash<auto> consth;

        #! map of constant runtime fields
        hash<auto> rconsth;

        #! map of field keys provided by the "runtime_keys" option
        hash<string, hash<MapperRuntimeKeyInfo>> runtime_keys;

        #! subset of runtime_keys with handlers
        hash<string, hash<auto>> runtime_keys_with_handler;

        #! hash of runtime keys that provide independent mappings (where there is no "requires_role" = "value")
        hash<string, hash<MapperRuntimeKeyInfo>> runtime_independent_keys;

        #! hash of valid keys
        hash<string, bool> valid_keys;

        #! mapper handler context hash; to be passed to handlers
        hash<auto> mapper_handler_context = {};

        #! mapper static thread context hash
        *hash<auto> mapper_thread_context;

        #! global submapper hash
        *hash<auto> global_submappers;

        #! maps standard types to auto types to avoid type stripping
        const TypeMap = {
            "hash": "hash<auto>",
            "*hash": "*hash<auto>",
            "list": "list<auto>",
            "*list": "*list<auto>",
            "softlist": "softlist<auto>",
            "*softlist": "*softlist<auto>",
        };

        #! maps deprecated transform options to global tranform options
        const DeprecatedGlobalTransformOptionMap = {
            "encoding": "string.encoding",
            "empty_strings_to_nothing": "string.empty_to_nothing",
            "date_format": "date.format",
            "timezone": "date.output_timezone",
            "input_timezone": "date.input_timezone",
            "number_format": "number.format",
        };

        #! Thread key for mapper thread context while mapping
        const _Mapper_Thread_Key = "_Mapper_Thread_Key";
    }

    #! builds the object based on a hash providing field mappings, data constraints, and optionally custom mapping logic
    /** @par Example:
        @code{.py}
const DataMap = (
# output field: "id" mapper from the "Id" element of any "^attributes^" hash in the input record
"id": "^attributes^.Id",
# output field: "name": maps from an input field with the same name (no translations are made)
"name": True,
# output field: "explicit_count": maps from the input "Count" field, if any value is present then it is converted to an integer
"explicit_count": ("type": "int", "name": "Count"),
# output field: "implicit_count": runs the given code on the input record and retuns the result, the code returns the number of "Products" sub-records
"implicit_count": int sub (any ignored, hash rec) { return rec.Products.size(); },
# output field: "order_date": converts the "OrderDate" string input field to a date in the specified format
"order_date": ("name": "OrderDate", "date_format": "DD.MM.YYYY HH:mm:SS.us"),
);

Mapper mapv(DataMap);
        @endcode

        @param mapv a hash providing field mappings; each hash key is the name of the output field; each value is either @ref True "True" (meaning no translations are done; the data is copied 1:1) or a hash describing the mapping; see @ref mapperkeys for detailed documentation for this option
        @param opts an optional hash of options for the mapper; see @ref mapperoptions for a description of valid mapper options

        @throw MAP-ERROR the map hash has a logical error (ex: \c "trunc" key given without \c "maxlen", invalid map key)
    */
    constructor(hash<auto> mapv, *hash<auto> opts) {
        setup(mapv, opts);

        # check map for logical errors
        checkMap();
    }

    #! private constructor for subclasses
    private constructor() {
    }

    static hash<string, AbstractDataField> getInputFromHash(hash<auto> input) {
        if (recordType.isAssignableFrom(input)) {
            return cast<hash<string, AbstractDataField>>(input);
        }
        return Mapper::getInputFromHashIntern(input);
    }

    static hash<string, AbstractDataField> getOutputFromHash(hash<auto> output, *hash<auto> mapv, *hash<auto> global_options) {
        if (recordType.isAssignableFrom(output)) {
            return cast<hash<string, AbstractDataField>>(output);
        }
        return Mapper::getOutputFromHashIntern(output, mapv, global_options);
    }

    #! Submits the data for processing by the mapper
    /** @param enqueue s closure taking a single arugment that enqueues the processed data for the next step in the
        pipeline; if no data should be processed onwards, do not call enqueue; if only one record should be processed
        onwards, then \a enqueue should be called only once; if multiple records are generated from the input data,
        then call it once for each generated record; prototype: @code{.py} code enqueue = sub (auto qdata) {} @endcode
        @param _data the data to process
    */
    private submitImpl(code enqueue, auto _data) {
        enqueue(mapAuto(_data));
    }

    #! Returns @ref True if the record processor supports bulk operation
    /** @return @ref True if the record processor supports bulk operation
    */
    private bool supportsBulkApiImpl() {
        return True;
    }

    #! Returns the expected type of data to be submitted, if available
    private *AbstractDataProviderType getExpectedTypeImpl() {
        if (input) {
            return new HashDataType(sprintf("mapper %y input record", name ?? "<unnamed>"), input);
        }
    }

    #! Returns the type of data that will be returned, if available
    private *AbstractDataProviderType getReturnTypeImpl() {
        if (output) {
            return new HashDataType(sprintf("mapper %y output record", name ?? "<unnamed>"), output);
        }
    }

    private AbstractDataProviderType getOutputType(hash<auto> hdesc, *hash<auto> mapdesc) {
        return Mapper::getOutputType(hdesc, mapdesc, global_transform_opts);
    }

    private static AbstractDataProviderType getOutputType(hash<auto> hdesc, *hash<auto> mapdesc, *hash<auto> global_options, *bool has_default_value) {
        *string fieldtype = hdesc.type ?? mapdesc.type;
        hash<auto> field_options;
        # use += here to ensure that field_options stays "hash<auto>"
        field_options += {
            "string.max_size_chars": hdesc.maxlen ?? mapdesc.maxlen,
            "number.format": hdesc.number_format ?? global_options."number.format",
            "date.format": hdesc.date_format ?? global_options."date.format",
            "date.input_timezone": global_options."date.input_timezone",
            "date.output_timezone": global_options."date.output_timezone",
            "string.encoding": global_options."string.encoding",
        };

        if (!fieldtype.val() || fieldtype == "any" || fieldtype == "auto") {
            # return types that support string.max_size_chars
            return hdesc.mand
                ? new SomethingDataType(field_options)
                : new AnythingDataType(field_options);
        }

        if (hdesc.mand && !exists field_options."qore.no_null" && !has_default_value) {
            field_options += {"qore.no_null": True};
        }

        switch (fieldtype) {
            case "number":
                return hdesc.mand
                    ? new QoreSoftNumberDataType(field_options)
                    : new QoreSoftNumberOrNothingDataType(field_options);

            case "int":
            case "integer":
                return hdesc.mand
                    ? new QoreSoftIntDataType(field_options)
                    : new QoreSoftIntOrNothingDataType();

            case "date": {
                return hdesc.mand
                    ? new QoreSoftDateDataType(field_options)
                    : new QoreSoftDateOrNothingDataType(field_options);
            }

            case "string": {
                return hdesc.mand
                    ? new QoreSoftStringDataType(field_options)
                    : new QoreSoftStringOrNothingDataType(field_options);
            }
        }

        Type type;
        if (fieldtype =~ /^\*/) {
            string base_field_type = fieldtype[1..];
            type = new Type(OptimalQoreSoftDataTypeMap{base_field_type} ?? base_field_type);
        } else {
            type = new Type(OptimalQoreSoftDataTypeMap{fieldtype} ?? fieldtype);
        }
        if (hdesc.mand && !has_default_value) {
            type = type.getBaseType();
        } else {
            type = type.getOrNothingType();
        }
        return AbstractDataProviderType::get(type, field_options);
    }

    private static hash<string, AbstractDataField> getInputFromHashIntern(hash<auto> input, *reference<bool> structured_input) {
        HashDataType input_data();
        foreach hash<auto> h in (input.pairIterator()) {
            switch (h.value.typeCode()) {
                case NT_STRING:
                    input_data.addField(new QoreDataField(h.key, h.value, AbstractDataProviderType::get(AutoType)));
                    break;
                case NT_NOTHING:
                case NT_HASH:
                    input_data.addField(new QoreDataField(h.key, h.value.desc, Mapper::getInputType(h.value.type),
                        h.value.default_value));
                    break;
                case NT_OBJECT:
                    if (h.value instanceof AbstractDataField) {
                        input_data.addField(h.value);
                        structured_input = True;
                        continue;
                    }
                    # continue to default and throw an exception
                default: throw "INPUT-OPTION-ERROR", sprintf("\"input\" key %y value has type %y (value %y); "
                    "expecting \"hash\"", h.key, h.value.type(), h.value);
            }
        }
        return input_data.getFields();
    }

    private static hash<string, AbstractDataField> getOutputFromHashIntern(hash<auto> output, *hash<auto> mapv, *hash<auto> global_options, *reference<bool> structured_output) {
        HashDataType output_data();
        foreach hash<auto> h in (output.pairIterator()) {
            if (h.value === True || !exists h.value) {
                output_data.addField(new QoreDataField(h.key, NOTHING, AutoType));
                continue;
            }
            if (h.value instanceof AbstractDataField) {
                output_data.addField(h.value);
                structured_output = True;
                continue;
            }
            switch (h.value.typeCode()) {
                case NT_STRING:
                    h.value = {"desc": h.value};
                    # fall down to next case
                case NT_HASH:
                    break;
                default:
                    throw "OUTPUT-OPTION-ERROR", sprintf("\"output\" key %y value has type %y (value %y); "
                        "expecting \"hash\"", h.key, h.value.type(), h.value);
            }
            foreach string k in (keys h.value) {
                if (!OutputKeys{k}) {
                    throw "OUTPUT-OPTION-ERROR", sprintf("\"output\" key %y hash has unknown key %y (valid keys: "
                        "%y)", h.key, k, keys OutputKeys);
                }
            }

            output_data.addField(new QoreDataField(h.key, h.value.desc,
                Mapper::getOutputType(h.value, mapv{h.key}.typeCode() == NT_HASH ? mapv{h.key} : NOTHING,
                    global_options, exists h.value.default_value),
                h.value.default_value));
        }
        return output_data.getFields();
    }

    private checkInputOutputOption(hash<auto> opts, string var, *hash<auto> mapv) {
        string structured = "structured_" + var;
        if (recordType.isAssignableFrom(opts{var})) {
            self{var} = opts{var};
            self{structured} = True;
            return;
        }

        if (opts{var}.typeCode() != NT_HASH) {
            error("%y option passed to %s::constructor() is not type \"hash\"; got type %y instead", var,
                self.className(), opts.input.type());
        }

        try {
            if (var == "input") {
                input = getInputFromHashIntern(opts.input, \structured_input);
            } else {
                output = getOutputFromHashIntern(opts.output, mapv, global_transform_opts, \structured_output);
            }
        } catch (hash<ExceptionInfo> ex) {
            error("%s: %s", ex.err, ex.desc);
        }
    }

    private setInputProvider(hash<auto> opts) {
        if (!(opts.input_provider instanceof AbstractDataProvider)) {
            error("\"input_provider\" option passed to %s::constructor() is not an instance of class "
                "\"AbstractDataProvider\"; got type %y instead", self.className(),
                opts.input_provider.type());
        }

        if (opts.input) {
            if (info_log) {
                info_log("both \"input\" and \"input_provider\" options passed to %s::constructor(), the \"input\" "
                    "option will be ignored", self.className());
            }
        }

        if (opts.input_provider.supportsRead()) {
            input_provider = opts.input_provider;
            input = input_provider.getRecordType();
            if (opts.input_provider.supportsRequest()) {
                input_request_search = True;
                input_request = opts.input_request;
                input_request_options = opts.input_request_options;
            }
        } else if (opts.input_provider.supportsRequest()) {
            input_provider = opts.input_provider;
            if (opts.input_response_error) {
                input = input_provider.getErrorResponseType(opts.input_response_error);
            } else {
                input = getFields("input", input_provider.getResponseType());
            }
            input_do_request = True;
            input_request = opts.input_request;
            input_request_options = opts.input_request_options;
        } else {
            error("\"input_provider\" %y passed to %s::constructor() does not support reading or the request/response "
                "API", opts.input_provider.getName(), self.className());
        }

        if (!exists input) {
            error("\"input_provider\" %y passed to %s::constructor() has an empty record type",
                input_provider.getName(), self.className());
        }

        if (opts.input_provider_search) {
            if (opts.input_provider_search.typeCode() != NT_HASH) {
                error("\"input_provider_search\" option passed to %s::constructor() is not type \"hash\"; "
                    "got type %y instead", self.className(), opts.input_provider_search.type());
            }
            input_provider_search = opts.input_provider_search;
            input_search_options = opts.input_search_options;
        }
    }

    private:internal *hash<string, AbstractDataField> getFields(string pfx, AbstractDataProviderType type) {
        *hash<string, AbstractDataField> fields = type.getFields();
        if (!fields && pfx != "input") {
            error("%s provider provides type %y which cannot be mapped", pfx, type.getName());
        }
        return fields ?? {};
    }

    private setOutputProvider(hash<auto> opts) {
        if (!(opts.output_provider instanceof AbstractDataProvider)) {
            error("\"output_provider\" option passed to %s::constructor() is not an instance of class "
                "\"AbstractDataProvider\"; got type %y instead", self.className(),
                opts.output_provider.type());
        }

        if (opts.output) {
            if (info_log) {
                info_log("both \"output\" and \"output_provider\" options passed to %s::constructor(), the \"output\" "
                    "option will be ignored", self.className());
            }
        }

        if (opts.output_provider.supportsCreate()) {
            output_provider = opts.output_provider;
            output = output_provider.getSoftRecordType();
            if (!output) {
                error("\"output_provider\" %y passed to %s::constructor() has an empty record type",
                    output_provider.getName(), self.className());
            }
            if (opts.output_provider_upsert) {
                if (!opts.output_provider.supportsUpsert()) {
                    error("\"output_provider\" %y passed to %s::constructor() does not support upserting",
                        output_provider.getName(), self.className());
                }
                if (opts.output_provider_bulk && !disable_bulk) {
                    output_provider_bulk_operation = output_provider.getBulkUpserter();
                }
                output_provider_upsert = True;
            } else {
                if (opts.output_provider_bulk) {
                    output_provider_bulk_operation = output_provider.getBulkInserter();
                }
            }
        } else if (opts.output_provider.supportsRequest()) {
            output_provider = opts.output_provider;
            output_do_request = True;
            output_request_options = opts.output_request_options;
            *AbstractDataProviderType type = opts.output_provider.getRequestType();
            if (type) {
                type = type.getSoftType();
                output = type.getFields();
            }
        } else {
            error("\"output_provider\" %y passed to %s::constructor() does not support writing or the "
                "request/response API", opts.output_provider.getName(), self.className());
        }
    }

    #! sets up the mapper object before checking the mapper hash
    private setup(hash<auto> mapv, *hash<auto> opts) {
        name = opts.name;
        info_log = opts.info_log;
        input_log = opts.input_log;
        output_log = opts.output_log;
        submapper_get = opts.submapper_get;
        template_subst = opts.template_subst;

        if (opts.submappers) {
            if (opts.submappers.typeCode() != NT_HASH) {
                error("the \"submappers\" option was provided as type %y; expecting \"hash\"",
                    opts.submappers.type());
            }
            if (!submapper_get) {
                error("the \"submappers\" option was provided without the \"submapper_get\" option (%y)",
                    opts.submappers);
            }
            map error("submappers option key %y has a value of type %y; expecting \"hash\" (%y)", $1.key,
                $1.value.type(), $1.value), opts.submappers.pairIterator(), $1.value.typeCode() != NT_HASH;

            global_submappers = opts.submappers;
        }

        if (opts.output_create_ignore_duplicates) {
            output_create_ignore_duplicates = True;
        }

        # set global transform options
        global_transform_opts += (map {
            DeprecatedGlobalTransformOptionMap{$1.key}: $1.value,
        }, opts{keys DeprecatedGlobalTransformOptionMap}.pairIterator());
        global_transform_opts += opts.global_type_options;

        if (opts) {
            checkTimezoneOption("input_timezone");
            checkTimezoneOption("output_timezone");
        }

        # set this before setting the output provider
        if (opts.disable_bulk) {
            disable_bulk = True;
        }

        # validate input record definition
        if (opts.input_provider) {
            setInputProvider(opts);
        } else if (opts.input) {
            checkInputOutputOption(opts, "input");
        }

        # validate output record definition
        if (opts.output_provider) {
            setOutputProvider(opts);

            opts.runtime_keys += output_provider.getMapperRuntimeKeys();
        } else if (opts.output) {
            checkInputOutputOption(opts, "output", mapv);
        }

        if (opts.trunc_all)
            trunc_all = parse_boolean(opts.trunc_all);

        if (opts.ignore_missing_input) {
            ignore_missing_input = True;
        }

        # setup mapper handler context
        if (opts.mapper_handler_context) {
            mapper_handler_context = opts.mapper_handler_context;
        }

        if (!mapv)
            error("empty map passed to %s::constructor()", self.className());

        mapc = mapv;
        {
            # check for invalid option keys
            list<string> all_keys = keys optionKeys();
            *hash<auto> invalid_option_keys = opts - all_keys;
            if (invalid_option_keys) {
                error("unknown option key%s %y passed to %s::constructor(); recognized option keys: %y",
                    invalid_option_keys.size() == 1 ? "" : "s", keys invalid_option_keys, self.className(),
                    all_keys);
            }
        }

        if (opts.allow_dot) {
            allow_dot = parse_boolean(opts.allow_dot);
            if (allow_dot && structured_input) {
                error("the deprecated 'allow_dot' option cannot be used with an input field description");
            }
        }

        if (opts.allow_output_dot) {
            allow_output_dot = parse_boolean(opts.allow_output_dot);
            if (allow_output_dot && structured_output) {
                error("the deprecated 'allow_output_dot' option cannot be used with an output field description");
            }
        }

        # runtime options
        if (opts.runtime) {
            if (opts.runtime.typeCode() != NT_HASH) {
                error("option 'runtime' passed to %s::constructor() assigned to type %y (value %y); expecting \"hash\"",
                        self.className(), opts.runtime.type(), opts.runtime);
            }
            # the "{} + " construction is there to make sure m_runtime has type hash<auto>
            m_runtime = {} + opts.runtime;
        }

        # validate and set runtime types
        if (opts.runtime_keys) {
            if (opts.runtime_keys.typeCode() != NT_HASH) {
                error("option 'runtime_keys' passed to %s::constructor() assigned to type %y (value %y); "
                    "expecting \"hash<string, hash<MapperRuntimeKeyInfo>>\"", self.className(),
                    opts.runtime.type(), opts.runtime);
            }
            foreach hash<auto> i in (opts.runtime_keys.pairIterator()) {
                if (i.value.fullType() != "hash<MapperRuntimeKeyInfo>") {
                    # try to cast for older code
                    bool ok;
                    if (i.value.typeCode() == NT_HASH) {
                        try {
                            if (i.value = cast<hash<MapperRuntimeKeyInfo>>(i.value)) {
                                ok = True;
                            }
                        } catch (hash<ExceptionInfo> ex) {
                        }
                    }

                    if (!ok) {
                        error("option 'runtime_keys' key %y passed to %s::constructor() has type %y; expecting "
                            "\"hash<MapperRuntimeKeyInfo>\"", i.key, self.className(), i.value.fullType());
                    }
                }
                runtime_keys{i.key} = i.value;
                # check required roles
                *hash<string, bool> role_hash = map {$1: True}, i.value.requires_roles;
                if (!role_hash.value) {
                    runtime_independent_keys{i.key} = i.value;
                }
                if (i.value.handler) {
                    runtime_keys_with_handler{i.key} = i.value + {
                        "requires_roles": role_hash,
                    };
                }
            }
        }

        if (opts.output_nullable) {
            output_nullable = True;
        }

        # set valid keys
        valid_keys = validKeys();
    }

    #! verifies the input map in the constructor
    private checkMap() {
        map checkMapField($1, \mapc{$1}), keys mapc;
        if (output_nullable) {
            setNullableOutput();
        }
        mapd = mapc;
        if (identh) {
            identl = keys identh;
            mapd -= identl;
        }
        if (consth) {
            mapd -= keys consth;
        }
        if (rconsth) {
            mapd -= keys rconsth;
        }
    }

    #! convert a field definition to a hash if possible
    private convertToHash(int t, string k, reference<auto> fh) {
        switch (t) {
            # convert to a hash if the value of the column is a string (giving the source key name)
            case NT_STRING: fh.name = fh; break;
            case NT_CALLREF:
            case NT_CLOSURE: fh.code = fh; break;
            case NT_BOOLEAN: if (fh) {
                fh = {};
                break;
            }
            case NT_NOTHING: {
                fh = {};
                break;
            }
            default: error("unsupported type %y assigned to output field %y", fh.type(), getFieldName(k));
        }
    }

    #! raises an error if an invalid input field name is declared; only call this if "input" is defined
    private checkInputField(string k, string name) {
        string n = allow_dot ? name : name.split('.')[0];
        if (!input{n})
            error("output field %y requires unknown input field %y; valid input fields are: %y", getFieldName(k),
                name, keys input);
    }

    #! perform per-field pre-processing on the passed map in the constructor
    /** @param k the field name
        @param fh a reference to the field's value in the map; will be converted to a hash
    */
    private checkMapField(string k, reference<auto> fh) {
        *AbstractDataField field;

        # check field description
        int t = fh.typeCode();
        if (t != NT_HASH)
            convertToHash(t, k, \fh);
        else if (fh.fullType() != "hash<auto>") {
            fh = {} + fh;
        }
        # save a copy of the original field has to use when checking field keys
        hash<auto> orig_fh = fh;

        # check output name
        if (output) {
            if (!output.hasKey(k)) {
                # see if field has a "." in it
                *string basename = (k =~ x/([^\.]+)/)[0];
                if (exists basename && (field = output{basename}) && field.isAssignableFrom(HashType)) {
                    # set key path for hash output fields
                    fh.output_key_path = k.split(".");
                    k = basename;
                } else {
                    error("output field %y is not a defined output field; known output fields: %y", getFieldName(k), keys output);
                }
            }
            if (!field) {
                # get output definition, if any
                field = output{k};
            }
        }

        # do we have an output field providing a value?
        bool have_output_value;
        foreach string key in (keys fh) {
            *hash<MapperRuntimeKeyInfo> mki = MapperKeyInfo{key} ?? runtime_keys{key};
            if (mki.handler || mki.unique_roles[0] == "value" || mki.unique_roles[0] == "*") {
                have_output_value = True;
                break;
            }
        }

        # process "name" key
        if (fh.name) {
            if (fh.struct)
                error("output field %y has both 'name' (%y) and 'struct' (%y) values; only one can be given to identify the input field", getFieldName(k), fh.name, fh.struct);
            if (input && !fh.hasKey("constant") && !fh.code && !exists fh.index && !exists fh.input_record)
                checkInputField(k, fh.name);
            if (!allow_dot && fh.name =~ /\./)
                fh.struct = (remove fh.name).split(".");
            else {
                # add to ident list if the input and output fields are identical
                if (fh.name == k && !fh.date_format && !fh.number_format && !fh.trunc && !trunc_all && !fh.subclass && !fh.code)
                    identh{k} = True;
            }
        } else if (fh.runtime) {
            if (!m_runtime.hasKey(fh.runtime))
                error("output field %y requires unregistered runtime key %y", getFieldName(k), fh.runtime);
        } else if (!fh."code" && !have_output_value && input && !fh.output_key_path) {
            checkInputField(k, k);
        }

        switch (fh.struct.typeCode()) {
            case NT_NOTHING: break;
            case NT_STRING: fh.struct = fh.struct.split("."); # then fall down to next case
            case NT_LIST: {
                if (!fh.struct)
                    error("output field %y has an empty 'struct' key", getFieldName(k));
                if (fh.struct.size() == 1) {
                    fh.name = (remove fh.struct)[0];
                    if (input) {
                        checkInputField(k, fh.name);
                    }
                } else if (input && !exists fh.constant && !fh.code && !exists fh.index) {
                    checkInputField(k, (foldl $1 + "." + $2, fh.struct));
                }
                break;
            }
            default: error("output field %y has an invalid struct key assigned to type %y (%y)", getFieldName(k), fh.struct.type(), fh);
        }

        bool explicit_type = exists fh.type;
        string implicit_type_source;

        {
            *string date_format = fh.date_format ?? fh.type_options."date.format";
            if (date_format) {
                if (date_format.typeCode() != NT_STRING)
                    error("field %y has a 'date_format' key assigned to type '%s'; expecting 'string'",
                        getFieldName(k), date_format.type());
                if (exists fh.type) {
                    if (fh.type != "date")
                        error("field %y has a 'date_format' key but the field's type is '%s'", getFieldName(k),
                            fh.type);
                } else {
                    fh.type = "date";
                    implicit_type_source = "\"date_format\" key";
                }
            }
        }

        {
            *string number_format = fh.number_format ?? fh.type_options."number.format";
            if (number_format) {
                if (number_format.typeCode() != NT_STRING)
                    error("field %y has a 'number_format' key assigned to type '%s'; expecting 'string'",
                        getFieldName(k), number_format.type());
                if (exists fh.type) {
                    if (fh.type != "number")
                        error("field %y has a 'number_format' key but the field's type is '%s'", getFieldName(k),
                            fh.type);
                } else {
                    fh.type = "number";
                    implicit_type_source = "\"number_format\" key";
                }
            }
        }

        # check declared type
        {
            foreach string key in (keys fh) {
                *hash<MapperRuntimeKeyInfo> mki = MapperKeyInfo{key} ?? runtime_keys{key};
                if (mki.returns_type) {
                    if (fh.type) {
                        string field_type = fh.type;
                        if (field_type =~ /^\*/) {
                            field_type = field_type[1..];
                        }
                        if (mki.returns_type != field_type) {
                            error("field %y has an %s type %y%s, but output key %y implies incompatible type %y",
                                getFieldName(k), explicit_type ? "explicit" : "implicit", fh.type,
                                implicit_type_source ? sprintf(" (derived from the %s)", implicit_type_source) : "",
                                key, mki.returns_type);
                        }
                    } else {
                        fh.type = mki.returns_type;
                        implicit_type_source = sprintf("%y key", key);
                    }
                }
            }
        }

        # check for contradictory definitions and assign values according to the output definition
        if (field) {
            if (exists fh.mand && fh.mand != field.isMandatory()) {
                error("field %y has the \"mand\" key set to %y but the output definition has mandatory flag: %y",
                    getFieldName(k), fh.mand, field.isMandatory());
            }
            if (exists fh.maxlen && fh.maxlen != field.getOptionValue("string.max_size_chars")) {
                error("field %y has the \"maxlen\" key set to %y but the output definition has a maximum size of "
                    "%y", getFieldName(k), fh.maxlen, field.getOptionValue("string.max_size_chars"));
            }
            # check type compatibility
            if (exists fh.type && !field.isAssignableFrom(new Type(fh.type))) {
                error("field %y has the \"type\" key set to %y but the output definition has a type of "
                    "to %y, and the types are not compatible; remove or correct the type in the mapper "
                    "definition to correct this error", getFieldName(k), fh.type, field.getTypeName());
            }
            # check number_format
            if (fh.number_format) {
                if (!(field.getType() instanceof QoreNumberDataTypeBase)) {
                    error("field %y has a 'number_format' key but the field's type is '%s'", getFieldName(k),
                        field.getTypeName());
                }
                # override any existing number format
                cast<QoreNumberDataTypeBase>(field.getType()).setOption("number.format", fh.number_format);
            }
            # check date format
            if (fh.date_format) {
                if (!(field.getType() instanceof QoreDateDataTypeBase)) {
                    error("field %y has a 'date_format' key but the field's type is '%s'", getFieldName(k),
                        field.getTypeName());
                }
                # override any existing date format
                cast<QoreDateDataTypeBase>(field.getType()).setOption("date.format", fh.date_format);
            }

            # allow the field hash to override any output default value
            if (exists fh."default") {
                field.setDefaultValue(fh."default");
            }
        } else {
            # create field object based on mapper hash
            field = new QoreDataField(k, fh.desc, getOutputType(fh), fh."default");
        }

        if (fh.trunc && field.getOptionValue("string.max_size_chars") < 0) {
            error("output field %y has the 'trunc' key set to True but has no 'maxlen' key", getFieldName(k));
        }

        if (field.getOptionValue("string.max_size_chars") >= 0 && !exists fh.trunc && trunc_all) {
            fh.trunc = True;
        }

        # set field options
        if (fh.type_options) {
            field.setOptions(fh.type_options);
        }

        # check valid keys
        {
            hash<string, bool> all_valid_keys = valid_keys;
            # add runtime keys, if any
            map all_valid_keys{$1} = True, keys runtime_keys;
            *hash<auto> invalid_keys = fh - (keys all_valid_keys);
            if (invalid_keys) {
                error("output field %y in map hash contains unknown key%s %y (valid keys: %y)",
                    getFieldName(k), invalid_keys.size() == 1 ? "" : "s", keys invalid_keys, keys all_valid_keys);
            }
        }

        # check runtime keys
        {
            hash<string, string> conflicts;
            hash<auto> key_config = runtime_keys + MapperKeyInfo;

            foreach hash<auto> i in (key_config{keys orig_fh}.pairIterator()) {
                # check for conflicts
                foreach string role in (i.value.unique_roles) {
                    if (role == "*" && orig_fh.size() > 1) {
                        error("output field %y key %y must be the only field in the output field mapping hash, "
                            "however the following keys are present: %y", getFieldName(k), i.key, keys (orig_fh - i.key));
                    }
                    if (conflicts{role}) {
                        error("output field %y has key %y with unique role %y, but key %y with the same unique "
                            "role has already been provided for this field.  Only one key with the same role can "
                            "be provided in the output field mapping hash: %y", getFieldName(k), i.key, role,
                            getFieldName(conflicts{role}));
                    }
                    conflicts{role} = i.key;
                }

                # check type
                if (i.value.value_type != "any") {
                    string type = i.value.value_type;
                    if (type[0] == "*") {
                        if (!exists orig_fh{i.key}) {
                            continue;
                        }
                        type = type[1..];
                    }
                    if (type == "int") {
                        type = Type::Int;
                    } else if (type == "option_hash") {
                        type = Type::Hash;
                    }
                    string vtype = orig_fh{i.key}.type();
                    if (type != vtype) {
                        # code == "closure" and "call reference"
                        if (type != "mapper-code" || (vtype != "closure" && vtype != "call reference")) {
                            error("output field %y key %y has value %y with type %y which is not compatible with the "
                                "expected type %y", getFieldName(k), i.key, orig_fh{i.key}, orig_fh{i.key}.type(), type);
                        }
                    }
                }
            }

            # check required roles
            foreach hash<auto> i in (key_config{keys orig_fh}.pairIterator()) {
                map error("output field %y key %y requires missing role/key %y", getFieldName(k), i.key, $1), i.value.requires_roles, !exists conflicts{$1};
            }
        }

        if (fh.submapper && !submapper_get) {
            error("output field %y requires submapper %y, but no \"submapper_get\" option was provided to the mapper",
                getFieldName(k), fh.submapper);
        }

        # convert old "number" tag to new "type" tag
        if (fh.number) {
            if (exists fh.type)
                error("output field %y has both 'type' and deprecated 'number' tags", getFieldName(k));
            if (!field.getType().isEqual(NumberType)) {
                error("output field %y type %y cannot be converted to \"number\" by the deprecated 'number' tag",
                    getFieldName(k), field.getTypeName());
            }
            delete fh.number;
        }

        if (exists fh.code && !fh.code.callp()) {
            error("output field %y has a code argument assigned to type '%s'", getFieldName(k), fh.code.type());
        }

        # check if the output field should be a hash
        if (!allow_output_dot && k =~ /\./) {
            fh.ostruct = k.split(".");
            mapo{fh.ostruct[0]} = NOTHING;
        } else {
            # add to consth if a constant value is included
            # refs #1754 if constant is 0, we cannot simply check with if(fh.constant)
            if (exists fh.constant)
                consth{k} = fh.constant;
            if (exists fh.runtime)
                rconsth{k} = fh.runtime;
            mapo{k} = NOTHING;
        }

        fh.field = field;
    }

    #! verifies a timezone constructor option
    private checkTimezoneOption(string rn) {
        auto val = global_transform_opts{rn};
        if (!exists val)
            return;
        if (val instanceof TimeZone) {
            global_transform_opts{rn} = val;
            return;
        }

        switch (val.typeCode()) {
            case NT_STRING:
            case NT_INT: global_transform_opts{rn} = new TimeZone(val); break;
            default: error("type %y assigned to the %s option (expecting TimeZone, string, or int)", val.type(), rn);
        }
    }

    #! set the @ref mapper_runtime_handling "runtime option" with \a "key" to value \a "value"
    /** @param key a string with valid runtime key
        @param value anything passed to the current runtime \c key

        @note this method cannot be called once data has been supplied to the Mapper; this method can only be used to initialize the Mapper with constant mappings before the Mapper processes any data

        @see
        - @ref mapper_runtime_handling
        - replaceRuntime()
        - setRuntime()

        @since %Mapper 1.1

        @deprecated use @ref constructor() "Mapper construction options" to set
                    the runtime options.

        @throw RUNTIME-OPTION-CHANGED the option has changed during while already
                processing the data.
    */
    setRuntime(string key, auto value) {
        if (count > 0 && exists m_runtime{key} && m_runtime{key} != value) {
            throw "RUNTIME-OPTION-CHANGED", sprintf("the runtime option %y "
                    "has changed %y -> %y while already %y input rows have been "
                    "processed", key, m_runtime{key}, value, count);
        }
        m_runtime{key} = value;
    }

    #! adds @ref mapper_runtime_handling "runtime options" to the current runtime option hash
    /**
        @param runtime a hash of runtime options to add to the current @ref mapper_runtime_handling "runtime option hash"

        @note this method cannot be called once data has been supplied to the Mapper; this method can only be used to initialize the Mapper with constant mappings before the Mapper processes any data

        @see
        - @ref mapper_runtime_handling
        - replaceRuntime()
        - setRuntime()

        @since %Mapper 1.1

        @deprecated use @ref constructor() "Mapper construction options" to set
                    the runtime options.

        @throw RUNTIME-OPTION-CHANGED the option has changed during while already
                processing the data.
    */
    setRuntime(hash<auto> runtime) {
        if (count > 0 && m_runtime + runtime != m_runtime) {
            throw "RUNTIME-OPTION-CHANGED", sprintf("the runtime options "
                    "has changed %y -> %y while already %y input rows have been "
                    "processed", m_runtime, m_runtime + runtime, count);
        }
        m_runtime += runtime;
    }

    #! replaces @ref mapper_runtime_handling "runtime options"
    /**
        @param runtime a hash of runtime options to use to replace the current @ref mapper_runtime_handling "runtime option hash"

        @note this method cannot be called once data has been supplied to the Mapper; this method can only be used to initialize the Mapper with constant mappings before the Mapper processes any data

        @see
        - @ref mapper_runtime_handling
        - getRuntime()
        - setRuntime()

        @since %Mapper 1.1

        @deprecated use @ref constructor() "Mapper construction options" to set
                    the runtime options.

        @throw RUNTIME-OPTION-CHANGED the option has changed during while already
                processing the data.
    */
    replaceRuntime(*hash<auto> runtime) {
        if (count > 0 && runtime != m_runtime) {
            throw "RUNTIME-OPTION-CHANGED", sprintf("the runtime options "
                    "has changed %y -> %y while already %y input rows have been "
                    "processed", m_runtime, runtime, count);
        }
        # the "{} + " construction is there to make sure m_runtime has type hash<auto>
        m_runtime = {} + runtime;
    }

    #! get current @ref mapper_runtime_handling "runtime option" value for a key
    /**
        @param key the runtime option key
        @returns a runtime value if the key exists in the current @ref mapper_runtime_handling "runtime option hash" and is set

        @see
        - @ref mapper_runtime_handling
        - replaceRuntime()
        - setRuntime()

        @since %Mapper 1.1
    */
    auto getRuntime(string key) {
        return m_runtime{key};
    }

    #! returns a descriptive name of the given field if possible, otherwise returns the field name itself
    string getFieldName(string fname) {
        return name ? sprintf("%s.%s", name, fname) : fname;
    }

    #! returns a list of valid field keys for this class (can be overridden in subclasses)
    /** @return a list of valid field keys for this class (can be overridden in subclasses)
    */
    hash<string, bool> validKeys() {
        return ValidKeys;
    }

    #! returns a list of valid constructor options for this class (can be overridden in subclasses)
    /** @return a list of valid constructor options for this class (can be overridden in subclasses)
    */
    hash<auto> optionKeys() {
        return OptionKeys;
    }

    #! returns mapper options useful for users
    hash<string, hash<MapperOptionInfo>> getUserOptions() {
        return UserOptions;
    }

    #! returns the input provider
    *AbstractDataProvider getInputProvider() {
        return input_provider;
    }

    #! returns the output provider
    *AbstractDataProvider getOutputProvider() {
        return output_provider;
    }

    #! returns the value of the \c "input" option
    *hash<string, AbstractDataField> getInputRecord() {
        return input;
    }

    #! returns the value of the \c "output" option
    *hash<string, AbstractDataField> getOutputRecord() {
        return output;
    }

    #! sets all fields in the output record as nullable
    /** @since %Mapper 1.5.1
    */
    setNullableOutput() {
        # update output field types
        output = map {$1.key: $1.value.getOrNothingType()}, output.pairIterator();
        # update types in "mapc"
        map mapc{$1}.field = mapc{$1}.field.getOrNothingType(), keys mapc;
    }

    #! returns an output record iterator that produces mapped data from the input data provider
    /** @return an output record iterator that produces mapped data from the input data provider

        @throw MAPPER-INPUT-PROVIDER-ERROR if no \a input_provider option was provided in the constructor or the input
        provider uses the request/response API

        @since %Mapper 1.5
    */
    MapperOutputRecordIterator getOutputIterator() {
        checkInputProvider();
        if (input_do_request) {
            error("cannot use an output iterator with an input provider using the request/response API");
        }
        return new MapperOutputRecordIterator(self, input_provider, input_provider_search, input_search_options);
    }

    #! Commits data written to the output data provider if the output data provider supports transaction management
    /** Has no effect if the output data provider does not support transaction management

        @throw MAPPER-OUTPUT-PROVIDER-ERROR no output provider available in this mapper

        @since %Mapper 1.5
    */
    commit() {
        checkOutputProvider();
        output_provider.commit();
    }

    #! Rolls back data written to the output data provider
    /** Has no effect if the output data provider does not support transaction management

        @throw MAPPER-OUTPUT-PROVIDER-ERROR no output provider available in this mapper

        @since %Mapper 1.5
    */
    rollback() {
        checkOutputProvider();
        output_provider.rollback();
    }

    #! Flushes any remaining data to the output data provider in a bulk operation
    /** This method should always be called for successful bulk output operations with an output provider

        @throw MAPPER-OUTPUT-BULK-ERROR no output bulk operation is in progress

        @see discardOutput()

        @since %Mapper 1.5
    */
    flushOutput() {
        checkOutputBulkOperation();
        if (output_provider_bulk_operation) {
            output_provider_bulk_operation.flush();
        }
    }

    #! Discards any buffered data in the output data provider in a bulk operation
    /** This method should always be called if an error occurs in a bulk output operation

        @throw MAPPER-OUTPUT-BULK-ERROR no output bulk operation is in progress

        @see flushOutput()

        @since %Mapper 1.5
    */
    discardOutput() {
        checkOutputBulkOperation();
        if (output_provider_bulk_operation) {
            output_provider_bulk_operation.discard();
        }
    }

    #! Runs the input and output mappers with data providers on each end autonomously
    /**
        @throw MAPPER-INPUT-PROVIDER-ERROR no input provider available in this mapper
        @throw MAPPER-OUTPUT-PROVIDER-ERROR no output provider available in this mapper

        @since %Mapper 1.5
    */
    runAutonomous() {
        checkInputProvider();
        checkOutputProvider();

        # first process autonomous mappings using the request/response API
        if (input_do_request) {
            if (info_log) {
                info_log("autonomous mapping %y request -> %y", input_provider.getName(), output_provider.getName());
            }

            auto response = input_provider.doRequest(input_request, input_request_options);
            mapData(response);
            return;
        }

        if (info_log) {
            if (input_request_search) {
                info_log("autonomous mapping %y request/search -> %y (bulk: %y type: %s)%s",
                    input_provider.getName(), output_provider.getName(), exists output_provider_bulk_operation,
                    output_do_request ? "req/resp" : "record", exists output_provider_bulk_operation ? (" using " +
                    output_provider_bulk_operation.className()) : NOTHING);
            } else {
                info_log("autonomous mapping %y (bulk: %y) -> %y (bulk: %y type: %s)%s",
                    input_provider.getName(), input_provider.supportsBulkRead(), output_provider.getName(),
                    exists output_provider_bulk_operation, output_do_request ? "req/resp" : "record",
                        exists output_provider_bulk_operation
                            ? (" using " + output_provider_bulk_operation.className())
                            : NOTHING);
            }
        }

        if (output_provider_bulk_operation) {
            on_error {
                output_provider_bulk_operation.discard();
            }
            on_success {
                output_provider_bulk_operation.flush();
            }
            if (input_provider.supportsBulkRead()) {
                # bulk read + bulk write for high performance mapping
                AbstractDataProviderBulkRecordInterface i = input_provider.searchRecordsBulk(NOTHING,
                    input_provider_search, input_search_options);
                while (*hash<auto> records = i.getValue()) {
                    mapBulk(records);
                }
            } else {
                # bulk output with normal input
                map mapData($1), getInputProviderRecordIterator();
            }
        } else {
            if (input_provider.supportsBulkRead()) {
                # bulk read + bulk write for high performance mapping
                AbstractDataProviderBulkRecordInterface i = input_provider.searchRecordsBulk(NOTHING,
                    input_provider_search, input_search_options);
                while (*hash<auto> records = i.getValue()) {
                    map mapData($1), records.contextIterator();
                }
            } else {
                # bulk output with normal input
                map mapData($1), getInputProviderRecordIterator();
            }
        }
    }

    #! Runs the input data provider through the mapper and returns the output
    /**
        @throw MAPPER-INPUT-PROVIDER-ERROR no input provider available in this mapper

        @since %Mapper 1.5.2
    */
    *softlist<hash<auto>> mapAutoInput() {
        checkInputProvider();

        # first process autonomous mappings using the request/response API
        if (input_do_request) {
            if (info_log) {
                info_log("autonomous input mapping %y request -> %y", input_provider.getName(), output_provider ? output_provider.getName() : "<no output provider>");
            }

            auto response = input_provider.doRequest(input_request, input_request_options);
            return mapData(response);
        }

        if (info_log) {
            if (input_request_search) {
                info_log("autonomous input mapping %y request/search", input_provider.getName());
            } else {
                info_log("autonomous input mapping %y records (bulk: %y)", input_provider.getName(),
                    input_provider.supportsBulkRead());
            }
        }

        if (output_provider_bulk_operation) {
            on_error {
                output_provider_bulk_operation.discard();
            }
            on_success {
                output_provider_bulk_operation.flush();
            }
            if (input_provider.supportsBulkRead()) {
                # bulk read + bulk write for high performance mapping
                AbstractDataProviderBulkRecordInterface i = input_provider.searchRecordsBulk(NOTHING,
                    input_provider_search, input_search_options);
                list<hash<auto>> rv;
                while (*hash<auto> records = i.getValue()) {
                    map rv += $1, mapBulk(records).contextItertor();
                }
                return rv;
            } else {
                # bulk output with normal input
                return map mapData($1), getInputProviderRecordIterator();
            }
        } else {
            if (input_provider.supportsBulkRead()) {
                # bulk read + bulk write for high performance mapping
                AbstractDataProviderBulkRecordInterface i = input_provider.searchRecordsBulk(NOTHING,
                    input_provider_search, input_search_options);
                while (*hash<auto> records = i.getValue()) {
                    return map mapData($1), records.contextIterator();
                }
            } else {
                # bulk output with normal input
                return map mapData($1), getInputProviderRecordIterator();
            }
        }
    }

    #! returns a record iterator for an input provider
    /** @since %Mapper 1.5
    */
    private:internal AbstractDataProviderRecordIterator getInputProviderRecordIterator() {
        if (input_request_search) {
            return input_provider.requestSearchRecords(input_request, input_provider_search, input_search_options);
        }
        return input_provider.searchRecords(input_provider_search, input_search_options);
    }

    #! maps all input records and returns the mapped data as a list of output records
    /** this method applies the @ref mapData() method to all input records and returns the resulting list
        @param recs the list of input records

        @return the mapped data as a list of output records

        @throw MISSING-INPUT a field marked mandatory is missing
        @throw STRING-TOO-LONG a field value exceeds the maximum value and the 'trunc' key is not set
        @throw INVALID-NUMBER the field is marked as numeric but the input value contains non-numeric data
    */
    list<hash<auto>> mapAll(list<auto> recs) {
        return map mapData($1), recs;
    }

    #! maps all input records and returns the mapped data as a list of output records
    /** this method applies the @ref mapData() method to all input records and returns the resulting list
        @param recs a hash of lists of input records

        @return the mapped data as a list of output records

        @throw MISSING-INPUT a field marked mandatory is missing
        @throw STRING-TOO-LONG a field value exceeds the maximum value and the 'trunc' key is not set
        @throw INVALID-NUMBER the field is marked as numeric but the input value contains non-numeric data
    */
    list<hash<auto>> mapAll(hash<auto> recs) {
        return map mapData($1), recs.contextIterator();
    }

    #! processes the input record and returns a hash of the mapped values where the keys in the hash returned are the target field names; the order of the fields in the hash returned is the same order as the keys in the map hash.
    /** @param rec the record to translate

        @return a hash of field values in the target format based on the input data and processed according to the logic in the map hash

        @throw MISSING-INPUT a field marked mandatory is missing
        @throw STRING-TOO-LONG a field value exceeds the maximum value and the 'trunc' key is not set
        @throw INVALID-NUMBER the field is marked as numeric but the input value contains non-numeric data

        @note
        - each time this method is executed successfully, the record count is updated (see @ref getCount() and @ref resetCount())
        - uses mapDataIntern() to map the data, then logOutput() is called for each output row
    */
    hash<auto> mapData(hash<auto> rec) {
        return mapDataIntern(rec, True);
    }

    #! maps all input record(s) automatically and returns the mapped data
    /** this method applies the @ref mapAll() method if the input records are given as list and the @ref mapBulk() method if the input is a hash of lists.
        For a simple hash (one record) it calls the @ref mapData() method.

        If @ref NOTHING is passed, then @ref NOTHING is returned with no mapping performed

        @param recs input records (one record or more), can be represented by list or hash

        @return the mapped data as of output records (type depends on the given records' type)

        @throw MISSING-INPUT a field marked mandatory is missing
        @throw STRING-TOO-LONG a field value exceeds the maximum value and the 'trunc' key is not set
        @throw INVALID-NUMBER the field is marked as numeric but the input value contains non-numeric data
        @throw MAPPER-INPUT-TYPE-ERROR records input type is not acceptable

        @since %Mapper 1.6
    */
    auto mapAuto(auto recs) {
        switch (recs.typeCode()) {
            case NT_LIST:
                return mapAll(recs);

            case NT_HASH:
                if (recs.firstValue().typeCode() == NT_LIST) {
                    # there are more records (hash of lists)
                    return disable_bulk ? mapAll(recs) : mapBulk(recs);
                }
                # only one record (simple hash)
                return mapData(recs);

            case NT_NOTHING:
                # do nothing in case no input was received
                return;
        }

        error2("MAPPER-INPUT-TYPE-ERROR", "mapAuto() does not accept the given type: %s,
            acceptable types are 'hash', 'list', or 'nothing'", recs.type());
    }

    #! maps a set of records in hash of lists format; returns mapped data in a hash of lists format
    /** @par Example:
        @code{.py}
        hash<auto> output = mapper.mapBulk(h);
        @endcode

        @param rec the input record or record set in case a hash of lists is passed
        @param crec an optional simple hash of data to be added to each input row before mapping

        @return returns a hash of lists of mapped data

        @note
        - using a hash of lists in \a rec; note that this provides very high performance when used with data
            provider output that support bulk write operations
        - in case a hash of empty lists is passed, Qore::NOTHING is returned
        - 'crec' does not affect the number of output lines; in particular, if 'rec' is a batch with \c N rows of
            a column \c C and 'crec = {"C": "mystring"}' then the output will be as if there was 'N' rows with
            \c C = "mystring" on the input.

        @see
        - flushOutput()
        - discardOutput()

        @throw MISSING-INPUT a field marked mandatory is missing
        @throw STRING-TOO-LONG a field value exceeds the maximum value and the 'trunc' key is not set
        @throw INVALID-NUMBER the field is marked as numeric but the input value contains non-numeric data
    */
    *hash<auto> mapBulk(hash<auto> rec, *hash<auto> crec) {
        if (global_submappers) {
            error2("MAP-BULK-ERROR", "bulk mapping cannot be performed with mappers with global submappers");
        }

        if (crec) {
            if (exists getRecListSize(crec)) {
                throw "MAPPER-BATCH-ERROR", sprintf("%s::queueData() does not accept a batch as its 2nd "
                    "argument, got %y", self.className(), crec);
            }
        }

        # NOTE: we cannot add 'crec' into 'rec' here since it could affect the number
        # of processed rows, actually, by overriding a column in 'rec' that
        # contains a list.
        *int rec_list_size = getRecListSize(rec);
        if (!exists rec_list_size) {
            return mapDataIntern(rec + crec);
        }

        if (rec_list_size > 0) {
            *hash<auto> old_ctx = swapMapperThreadContext(mapper_thread_context);
            on_exit swapMapperThreadContext(old_ctx);

            hash<auto> dh;

            if (crec) {
                # expand crec to have the same number of rows as rec in any key
                # that has already got a list value in crec. This is to avoid
                # situations that crec changes the number of rows really inserted.
                foreach hash<auto> i in (crec.pairIterator()) {
                    if (i.value.typeCode() != NT_LIST &&
                        (exists rec{i.key} &&
                            rec{i.key}.typeCode() == NT_LIST)) {
                        rec{i.key} = map i.value, xrange(0, rec_list_size);
                    } else {
                        rec{i.key} = i.value;
                    }
                }
            }

            # identl and rconsth entries are added to 'dh' and not in 'hbuf'
            # directly since identl and rconsth may change at each call of
            # mapBulk() (unlike consth).

            # first copy all 1:1 mappings to the output hash
            if (identl)
                dh += rec{identl};

            # copy all runtime mappings to the output hash
            map dh{$1.key} = m_runtime{$1.value}, rconsth.pairIterator();

            map mapFieldIntern(\dh, $1, rec, True, rec_list_size), keys mapd;

            # add constant values to hash
            dh += consth;

            # write to output provider, if any
            if (output_provider_bulk_operation) {
                output_provider_bulk_operation.queueData(dh);
            } else if (output_provider) {
                if (output_provider_upsert) {
                    map output_provider.upsertRecord($1), dh.contextIterator();
                } else {
                    if (output_do_request) {
                        map output_provider.doRequest($1, output_request_options), dh.contextIterator();
                    } else {
                        map doCreateRecordIntern($1), dh.contextIterator();
                    }
                }
            }

            count += rec_list_size;

            return dh;
        } # else no input, no work, not even for constant mappers
    }

    #! Get mapper thread context
    static *hash<auto> getMapperThreadContext() {
        return get_thread_data(_Mapper_Thread_Key);
    }

    #! processes the input record and returns a hash of the mapped values where the keys in the hash returned are the target field names; the order of the fields in the hash returned is the same order as the keys in the map hash.
    /** @param rec the record to translate

        @return a hash of field values in the target format based on the input data and processed according to the logic in the map hash

        @throw MISSING-INPUT a field marked mandatory is missing
        @throw STRING-TOO-LONG a field value exceeds the maximum value and the 'trunc' key is not set
        @throw INVALID-NUMBER the field is marked as numeric but the input value contains non-numeric data

        @note
        - each time this method is executed successfully, the record count is updated (see @ref getCount() and @ref resetCount())
        - this is the same as mapData() except no output logging is performed
    */
    private hash<auto> mapDataIntern(hash<auto> rec, *bool do_log_output) {
        *hash<auto> old_ctx = swapMapperThreadContext(mapper_thread_context);
        on_exit swapMapperThreadContext(old_ctx);

        if (input_log)
            input_log(rec);

        # hash of mapped data to be added to h
        hash<auto> h;
        # assigning after the declaration ensure that the hash stays "hash<auto>"
        # mapo provides the output field order
        h = mapo
            # first copy all 1:1 mappings to the output hash
            + (identl ? rec{identl} : NOTHING)
            # then copy all constant mappings to the output hash
            + consth
            # then copy all runtime constant mappings to the output hash
            + map {$1.key: m_runtime{$1.value}}, rconsth.pairIterator();

        # iterate through dynamic target fields
        map mapFieldIntern(\h, $1, rec, False, 0), keys mapd;

        # log output before any output provider
        if (do_log_output) {
            logOutput(h);
        }

        # write to putput provider, if any
        if (output_provider_bulk_operation) {
            output_provider_bulk_operation.queueData(h);
        } else if (output_provider) {
            if (output_provider_upsert) {
                output_provider.upsertRecord(h);
            } else {
                if (output_do_request) {
                    output_provider.doRequest(h, output_request_options);
                } else {
                    h = doCreateRecordIntern(h);
                }
            }
        }

        # execute global submappers; if any
        foreach hash<auto> i in (global_submappers.pairIterator()) {
            Mapper submapper = submapper_get(i.key, getOptionsFromTemplate(i.value, rec, h));
            submapper.mapAutoInput();
        }

        # increment record count
        ++count;

        return h;
    }

    #! Creates a record with the output data provider
    private *hash<auto> doCreateRecordIntern(hash<auto> rec) {
        try {
            return output_provider.createRecord(rec);
        } catch (hash<ExceptionInfo> ex) {
            if (ex.err == "DUPLICATE-RECORD" && output_create_ignore_duplicates) {
                if (info_log) {
                    info_log("duplicate record: %y; returning %y", rec, ex.arg);
                }
                return ex.arg;
            }
            rethrow;
        }
    }

    #! Swap the mapper thread context
    private static *hash<auto> swapMapperThreadContext(*hash<auto> new_ctx) {
        *hash<auto> rv = get_thread_data(_Mapper_Thread_Key);
        save_thread_data(_Mapper_Thread_Key, new_ctx);
        return rv;
    }

    /** For a possible hash of lists (bulk data) returns the size of
        * the first list found within the hash. If no list if found, returns
        * NOTHING.
        * @param rec a hash representing (possible) bulk data
        * @return NOTHING if the hash represents single input record, or the number
        *         of records represented (may also be 0)
        */
    private *int getRecListSize(hash<auto> rec) {
        foreach auto v in (rec.iterator()) {
            if (v.typeCode() == NT_LIST) {
                return v.size();
            }
        }
        return NOTHING;
    }

    #! maps a single field to the target
    /**
        * Performs the actual mapping
        *
        * @param h the hash to be updated with the mapped key/value pair;
        *          Depending on the mapper specification and the input, it can
        *          contain a bulk record (hash of lists) or a hash of both list
        *          and non-list keys (these are considered a "constants" in the
        *          output record.
        * @param key the column name (hash key) to be mapped (target field)
        * @param rec input record - either single record of hash of lists (batch);
        *                           to increase performance, the input type
        *                           (single record vs. batch) is determined by
        *                           the \a do_list parameter. In case of
        *                           bulk input, all the lists are supposed to
        *                           have the same length \a list_size
        * @param do_list - whether the input record \a rec is single record or
        *                  bulk format (hash of lists)
        * @param list_size - size of the lists in case the input is in bulk
        *                    format (all lists must have the same length,
        *                    asserted inside the method).
        *
        * @note it is assumed that list_size > 0 whenever do_list is Qore::True
        * @note if do_list == Qore::True, the 'rec' can contain a mix of lists
        *       and non-list values - the non-list values are used as constants
        *       in the mapping - as if expanded to lists with all values identical.
        */
    private nothing mapFieldIntern(reference<hash<auto>> h, string key, hash<auto> rec, bool do_list, int list_size) {
        # FIXME: assert(!do_list || list_size > 0);
        # FIXME can add the assert for equal length of the lists --PQ 22-Mar-2017
        hash<auto> m = mapc{key};

        #printf("mapFieldIntern() key: %y type: %y rec: %y\n", key, m.field.getTypeName(), rec);

        # closure to get the current record hash from a hash of lists;
        # rec is not bound here for performance reasons (so it will remain unlocked);
        # NOTE that we still need the NT_LIST check here since even with do_list
        # we can get constants in 'rec' (aka non-lists).
        code getrec = hash<auto> sub (hash<auto> rc, int offset) {
            return map {$1.key : $1.value.typeCode() == NT_LIST ?
                                    $1.value[offset] :
                                    $1.value}, rc.pairIterator();
        };

        # get source field name
        string name = m.name ?? key;

        # is the input missing?
        bool missing;

        # get source record value
        auto v;
        if (exists m.constant)
            v = m.constant;
        else if (exists m.index) {
            if (do_list) {
                v = map m.index + count + $#, xrange(0, list_size);
            } else {
                v = m.index + count;
            }
        } else if (exists m.runtime) {
            # actually runtime should not happen with do_list since
            # do_list = True is only used from TableMapper that is handling
            # runtime outside mapFieldIntern()
            v = m_runtime{m.runtime};
        } else if (m.struct) {
            v = rec;
            if (ignore_missing_input) {
                foreach string k in (m.struct) {
                    if (!v.hasKey(k)) {
                        missing = True;
                        remove v;
                        break;
                    }
                    v = v{k};
                }
                #printf("struct: %y v: %y missing: %y\n", m.struct, v, missing);
            } else {
                map v = v{$1}, m.struct;
            }
        } else if (m.use_input_record) {
            v = rec;
        } else {
            if (do_list && rec{name}.typeCode() == NT_LIST) {
                # must make sure that v does not take a complex list restriction here
                v = () + rec{name};
            } else {
                if (ignore_missing_input && !rec.hasKey(name)) {
                    missing = True;
                } else {
                    v += rec{name} ?? NOTHING; # mitigate NULL -> NOTHING
                }
            }
        }

        bool v_is_list = v.typeCode() == NT_LIST;
        if (v_is_list) {
            if (do_list) {
                if (list_size != v.size()) {
                    error2("MAPPER-FIELD-LIST-ERROR", "field %y value passed is the list %y with length %d - the "
                        "input batch length expected is %d", key, v, v.size(), list_size);
                }
                if (ignore_missing_input) {
                    error2("MAPPER-FIELD-LIST-ERROR", "cannot use the \"ignore_missing_input\" option with bulk "
                        "input");
                }
            } else {
                v_is_list = False;
            }
        }

        if (m.submapper) {
            if (v_is_list) {
                error2("MAPPER-FIELD-LIST-ERROR", "cannot use the \"submapper\" option with bulk input");
            }
            *hash<auto> submapper_options;
            if (m.submapper_options) {
                submapper_options += getOptionsFromTemplate(m.submapper_options, rec);
            }
            if (info_log) {
                info_log("field %y retrieving submapper %y (opts: %y auto input: %y)", key, m.submapper, submapper_options, m.submapper_auto_input ?? False);
            }
            if (input_log) {
                submapper_options.input_log = input_log;
            }
            if (output_log) {
                submapper_options.output_log = output_log;
            }
            if (info_log) {
                submapper_options.info_log = info_log;
            }
            Mapper submapper = submapper_get(m.submapper, submapper_options);
            if (m.submapper_auto_input) {
                v = submapper.mapAutoInput();
            } else {
                v = submapper.mapAll(v);
            }
        }

        # move any XML CDATA into the field value
        # do not access "^cdata^" directly in case it's a hashdecl
        if (v.typeCode() == NT_HASH && v.hasKey("^cdata^") && v."^cdata^".val())
            v = v."^cdata^";

        # if the internal field was marked as needing processing by a subclass, then call the mapSubclass method
        if (m.subclass)
            v = v_is_list ? (map mapSubclass(m, $1), v) : mapSubclass(m, v);

        # process any runtime keys if present in the current field mapping
        if (*hash<string, hash<auto>> current_runtime_keys = runtime_keys_with_handler{keys m}) {
            hash<auto> ctx = mapper_handler_context + {
                "output-key": key,
                "input": rec,
            };
            if (!do_list) {
                # optimized version in one expression
                try {
                    map v = $1.value.requires_roles.value
                        ? $1.value.handler(m{$1.key}, v, ctx, \missing)
                        : $1.value.handler(m{$1.key}, ctx, \missing),
                        current_runtime_keys.pairIterator();
                } catch (hash<ExceptionInfo> ex) {
                    error2(ex.err, "%s (field %y)", ex.desc, key);
                }
            } else {
                foreach hash<auto> i in (current_runtime_keys.pairIterator()) {
                    try {
                        if (do_list && (exists v || i.value.requires_roles.value)) {
                            v = map i.value.requires_roles.value
                                ? i.value.handler(m{i.key}, $1, ctx)
                                : i.value.handler(m{i.key}, ctx), v;
                        } else {
                            v = i.value.requires_roles.value
                                ? i.value.handler(m{i.key}, v, ctx)
                                : i.value.handler(m{i.key}, ctx);
                        }
                    } catch (hash<ExceptionInfo> ex) {
                        error2(ex.err, "%s (field %y mapping key %y)", ex.desc, key, i.key);
                    }
                }
            }
        }

        # execute any field filter if necessary
        if (m.code) {
            try {
                if (do_list) {
                    v = map m.code(v_is_list ? v[$#] : v, getrec(rec, $#)),
                        xrange(0, list_size);
                    # here we make a list from 'v' either way, since we simply
                    # cannot predict what 'code' could do with 'v' and 'rec'
                    v_is_list = True;
                } else {
                    v = m.code(v, rec);
                }
            } catch (hash<ExceptionInfo> ex) {
                ex.desc = sprintf("%y mapper field %y closure: %s", name, key, ex.desc);
                throw ex.err, ex.desc, ex.arg;
            }
        }

        *bool empty_strings_to_nothing = m.field.getOptionValue("string.empty_to_nothing")
            ?? global_transform_opts."string.empty_to_nothing";
        if (v_is_list) {
            map delete v[$#], v, (empty_strings_to_nothing && $1 === "" || $1 === NULL);
        } else {
            if ((empty_strings_to_nothing && v === "") || v === NULL)
                delete v;
        }

        if (m.field.hasType()) {
            *AbstractDataProviderType type = m.field.getType();
            if (m.output_key_path) {
                foreach string elem in (m.output_key_path[1..]) {
                    type = type.getFieldType(elem);
                    if (!type) {
                        break;
                    }
                }
            }
            if (type) {
                *hash<string, bool> direct_types = type.getDirectTypeHash();

                # note: v_is_list implies do_list
                if (v_is_list) {
                    map
                        v[$#] = mapFieldType(key, m, type, v[$#], getrec(rec, $#)),
                        v,
                        !direct_types{$1.typeCode()};
                } else if (!direct_types{v.typeCode()}) {
                    v = mapFieldType(key, m, type, v, rec);
                }
            }
        }

        if (exists (auto default_value = m.field.getDefaultValue())) {
            if (v_is_list) {
                map v[$#] = default_value, v, !exists $1;
            } else if (!exists v)
                v = default_value;
        }

        #printf("k: %y (okp: %y) type: %y maxlen: %y size: %y\n", key, m.output_key_path, m.field.getTypeName(), m.field.getOptionValue("string.max_size_chars"), v.size());

        # check maximum length
        if ((*int maxlen = m.field.getOptionValue("string.max_size_chars")) > 0) {
            if (v_is_list) {
                if (m.trunc)
                    map v[$1] = truncateField(key, $1, $#, v.size(), maxlen), v, $1.size() > maxlen;
                else
                    map fieldLengthError(key, $1, $#, v.size(), maxlen, getrec(rec, $#)), v, $1.size() > maxlen;
            } else {
                if (v.size() > maxlen) {
                    # truncate the string if necessary
                    if (m.trunc) {
                        v = truncateField(key, v, 0, 0, maxlen);
                    } else {
                        fieldLengthError(key, v, 0, 0, maxlen, rec);
                    }
                }
            }
        }

        if (m.mand) {
            if (v_is_list) {
                map error2("MISSING-INPUT", "field %y element %d/%d is marked as mandatory but is missing in the "
                    "input row: %y", getFieldName(key), $# + 1, v.size(), getrec(rec, $#)), v, !exists $1;
            } else if (!exists v)
                error2("MISSING-INPUT", "field %y is marked as mandatory but is missing in the input row: %y",
                    getFieldName(key), rec);
        } else if (ignore_missing_input && !exists v && missing) {
            # supressing output for field
            if (m.output_key_path) {
                removeHashOutputKey(\h, m.output_key_path, 0);
            } else {
                remove h{key};
            }
            return;
        }

        # add value to row list
        if (m.ostruct) {
            # recursive closure for generating structured data
            code ah = auto sub (*hash<auto> ch, auto vc, int off = 0) {
                if (off == m.ostruct.size())
                    return vc;
                string k = m.ostruct[off];
                auto oh = ch{k};
                if (exists oh && oh.typeCode() != NT_HASH)
                    throw "INVALID-OUTPUT", sprintf("field %y cannot overwrite element %y with type %y", getFieldName(key), k, oh.type());
                ch{k} = ah(oh, vc, off + 1);
                return ch;
            };
            try {
                h = ah(h, v);
                return;
            } catch (hash<ExceptionInfo> ex) {
                if (ex.err == "INVALID-OUTPUT")
                    error2(ex.err, ex.desc);
                else
                    rethrow;
            }
        }

        if (m.output_key_path) {
            # recursively write output hash
            writeHashOutput(\h, v, m.output_key_path, 0);
        } else {
            h{key} = v;
        }
    }

    #! Returns options from a template hash
    hash<auto> getOptionsFromTemplate(hash<auto> opts, hash<auto> input_rec, *hash<auto> output_rec) {
        if (template_subst) {
            return template_subst(opts, {
                "input": input_rec,
                "output": output_rec,
            });
        }
        hash<auto> new_opts();
        # do simple default template substitution
        foreach hash<auto> i in (opts.pairIterator()) {
            switch (i.value.typeCode()) {
                case NT_HASH:
                    new_opts{i.key} = getOptionsFromTemplate(i.value, input_rec, output_rec);
                    break;

                case NT_LIST:
                    new_opts{i.key} = map getOptionsFromTemplate($1, input_rec, output_rec), i.value;
                    break;

                case NT_STRING: {
                    bool found;
                    if (*list<string> vl = (i.value =~ x/\$local:input\.([a-z0-9_]+)/)) {
                        found = True;
                        new_opts{i.key} = i.value;
                        map new_opts{i.key} = replace(new_opts{i.key}, "$local:input." + $1, input_rec{$1}), vl;
                    }
                    if (output_rec && (*list<string> vl = (i.value =~ x/\$local:output\.([a-z0-9_]+)/))) {
                        found = True;
                        new_opts{i.key} = i.value;
                        map new_opts{i.key} = replace(new_opts{i.key}, "$local:output." + $1, output_rec{$1}), vl;
                    }
                    if (found) {
                        break;
                    }
                    # fall down to next case
                }

                default:
                    new_opts{i.key} = i.value;
                    break;
            }
        }
        return new_opts;
    }

    # recursively remove a key from the output
    private removeHashOutputKey(reference<auto> output, list<auto> path, int offset) {
        if (offset == path.size() - 1) {
            remove output{path[offset - 1]};
        } else {
            removeHashOutputKey(\output{path[offset]}, path, offset + 1);
        }
    }

    # recursively write output hash
    private writeHashOutput(reference<auto> output, auto value, list<auto> path, int offset) {
        if (offset == path.size()) {
            output = value;
        } else {
            writeHashOutput(\output{path[offset]}, value, path, offset + 1);
        }
    }

    #! called to truncate fields when processing hashes of lists
    private string truncateField(string k, string val, int ix, int sze, int maxlen) {
        if (info_log)
            info_log("field %y = %y%s input length %d truncating to %d bytes",
                    getFieldName(k), val, (sze > 0 ? sprintf(" element %d/%d", ix + 1, sze) : ""), val.size(), maxlen);
        return trunc_str(val, maxlen, global_transform_opts."string.encoding");
    }

    #! called when a field exceeds its maximum length when processing hashes of lists
    private fieldLengthError(string k, string val, int ix, int sze, int maxlen, hash<auto> rc) {
        error2("STRING-TOO-LONG", "field %y = %y%s, input length %d exceeds maximum byte length %d for input row: %y",
                getFieldName(k), val, (sze > 0 ? sprintf(" element %d/%d", ix + 1, sze) : ""), val.size(), maxlen, rc);
    }

    #! calls the output logging @ref closure "closure" or @ref call_reference "call reference" (if any) to log the output record
    logOutput(hash<auto> h) {
        if (output_log)
            output_log(h);
    }

    #! returns the internal record count
    /** @see resetCount()
    */
    int getCount() {
        return count;
    }

    #! resets the internal record count
    /** @see getCount()
    */
    resetCount() {
        count = 0;
    }

    #! performs type handling
    private auto mapFieldType(string key, hash<auto> mapping, AbstractDataProviderType type, auto value, hash<auto> rec) {
        try {
            return type.acceptsValue(value);
        } catch (hash<ExceptionInfo> ex) {
            throw ex.err, sprintf("%s: error in field %y with value %y; %s; rec: %y", get_ex_pos(ex), key, value, ex.desc, rec), ex.arg;
        }
    }

    #! raises an error if no input provider is present
    private checkInputProvider() {
        if (!input_provider) {
            error2("MAPPER-INPUT-PROVIDER-ERROR", "no input provider available in this mapper");
        }
    }

    #! raises an error if no output provider is present
    /** @throw MAPPER-OUTPUT-PROVIDER-ERROR no output provider available in this mapper
    */
    private checkOutputProvider() {
        if (!output_provider) {
            error2("MAPPER-OUTPUT-PROVIDER-ERROR", "no output provider available in this mapper");
        }
    }

    #! raises an error if no output provider is present
    /** @throw MAPPER-OUTPUT-BULK-ERROR no output bulk operation is in progress
    */
    private checkOutputBulkOperation() {
        if (!output_provider) {
            error2("MAPPER-OUTPUT-BULK-ERROR", "no output bulk operation is in progress");
        }
    }

    #! throws a \c MAP-ERROR exception; prepends the map name to the description if known
    /** if this method is subclassed, it must also cause an exception to be thrown
    */
    private error(string fmt) {
        string err = vsprintf(fmt, argv);
        if (name)
            err = sprintf("mapper %y: %s", name, err);
        throw "MAP-ERROR", err;
    }

    #! throws the given exception; prepends the map name to the description if known
    private error2(string ex, string fmt) {
        string err = vsprintf(fmt, argv);
        if (name)
            err = sprintf("%y mapper: %s", name, err);
        throw ex, err;
    }

    #! to be overridden as necessary in subclasses
    private auto mapSubclass(hash<auto> m, auto v) {
        return v;
    }

    static private AbstractDataProviderType getInputType(*string type) {
        if (!type.val() || type == "any") {
            return AbstractDataProviderType::get(AutoType);
        }
        return AbstractDataProviderType::get(new Type(TypeMap{type} ?? type));
    }
}

#! abstract base class for hash iterator mappping classes based on a mapper object and an iterator input source
public class AbstractMapperIterator inherits Qore::AbstractIterator {
    public {
    }

    private {
        #! input iterator; @ref Qore::AbstractIterator::getValue() "AbstractIterator::getValue()" must return a hash
        Qore::AbstractIterator i;
    }

    #! creates the iterator from the arguments passed
    /** @param iter input iterator; @ref Qore::AbstractIterator::getValue() "AbstractIterator::getValue()" must return a hash
    */
    constructor(Qore::AbstractIterator iter) {
        i = iter;
    }

    #! Moves the current position of the iterator to the next element; returns @ref False "False" if there are no more elements
    bool next() {
        return i.next();
    }

    #! returns @ref True "True" if the iterator is currently pointing at a valid element, @ref False "False" if not
    bool valid() {
        return i.valid();
    }

    #! returns @ref True if the iterator supports bulk mode; this method returns @ref False (the default)
    bool hasBulk() {
        return False;
    }

    #! performs bulk mapping; if the iterator does not support bulk mapping then it is simulated in this method
    /** @param size the number of rows to return

        @return a list of mapped hashes with a maximum number of rows corresponding to the \a size argument; in case there is less input data than requested, the list returned could have fewer rows than requested; in case there is no more data, the return value is an empty list
        */
    list<hash> mapBulk(int size) {
        list<hash> rv();
        while (next()) {
            rv += getValue();
            if (rv.size() == size)
                break;
        }
        return rv;
    }
}

#! provides a hash iterator based on a mapper object and an iterator input source
public class MapperIterator inherits Mapper::AbstractMapperIterator {
    public {
    }

    private {
        #! data mapper
        Mapper::Mapper mapc;
    }

    #! creates the iterator from the arguments passed
    /** @param i input iterator; @ref Qore::AbstractIterator::getValue() "AbstractIterator::getValue()" must return a hash
        @param mapv a hash providing field mappings; each hash key is the name of the output field; each value is either @ref True "True" (meaning no translations are done; the data is copied 1:1) or a hash describing the mapping; see @ref mapperkeys for detailed documnentation for this option
        @param opts an optional hash of options for the mapper; see @ref mapperoptions for a description of valid mapper options

        @throw MAP-ERROR the map hash has a logical error (ex: \c "trunc" key given without \c "maxlen", invalid map key)
        */
    constructor(Qore::AbstractIterator i, hash<auto> mapv, *hash<auto> opts) : Mapper::AbstractMapperIterator(i) {
        mapc = new Mapper(mapv, opts);
    }

    #! creates the iterator from the arguments passed
    /** @param i input iterator; @ref Qore::AbstractIterator::getValue() "AbstractIterator::getValue()" must return a hash
        @param mapv the mapper to transform the data
    */
    constructor(Qore::AbstractIterator i, Mapper::Mapper mapv) : Mapper::AbstractMapperIterator(i) {
        mapc = mapv;
    }

    #! returns the current row transformed with the mapper
    hash<auto> getValue() {
        return mapc.mapData(i.getValue());
    }

    #! returns the internal record count
    /** @see resetCount()
    */
    int getCount() {
        return mapc.getCount();
    }

    #! resets the internal record count
    /** @see getCount()
    */
    resetCount() {
        mapc.resetCount();
    }
}

#! describes a data type that accepts any value; stores "string.max_size_chars" as a type attribute for external enforcement
public class AnythingDataType inherits AbstractDataProviderType {
    private {
        #! supports a max_size_chars option for strings; to be enforced externally
        const SupportedOptions = {
            "string.max_size_chars": <DataProviderTypeOptionInfo>{
                "type": Type::Int,
                "desc": "the maximum size of strings in chars",
            },
            "string.empty_to_nothing": <DataProviderTypeOptionInfo>{
                "type": Type::Boolean,
                "desc": "if an empty string should be converted to no value",
            },
        };
    }

    #! creates the object
    constructor(*hash<auto> options) : AbstractDataProviderType(options) {
    }

    #! returns the type name
    string getName() {
        return "anything";
    }

    #! returns True if this type can be assigned from values of the argument type
    bool isAssignableFrom(AbstractDataProviderType t) {
        return True;
    }

    #! returns True if this type can be assigned from values of the argument type
    bool isAssignableFrom(Type t) {
        return True;
    }

    #! returns the base type for the type, if any
    *Type getValueType() {
        return AutoType;
    }

    #! returns the subtype (for lists or hashes) if there is only one
    *AbstractDataProviderType getElementType() {
        # this method intentionally left blank
    }

    #! returns the value if the value can be assigned to the type
    /** @param value the value to assign to the type

        @return the value to be assigned; can be converted by the type

        @throw RUNTIME-TYPE-ERROR value cannot be assigned to the type
    */
    auto acceptsValue(auto value) {
        if (value === "" && options."string.empty_to_nothing") {
            return;
        }
        return value;
    }

    #! returns the fields of the data structure; if any
    *hash<string, AbstractDataField> getFields() {
        # this method intentionally left blank
    }

    #! returns supported options
    *hash<string, hash<DataProviderTypeOptionInfo>> getSupportedOptions() {
        return SupportedOptions;
    }

    #! returns a hash of base types accepted by this type
    hash<string, bool> getAcceptTypeHash() {
        return {"all": True};
    }

    #! returns a hash of base types returned by this type
    hash<string, bool> getReturnTypeHash() {
        return {"all": True};
    }
}

#! describes a data type that accepts any value except NOTHING; stores "string.max_size_chars" as a type attribute for external enforcement
public class SomethingDataType inherits AnythingDataType {
    #! creates the object
    constructor(*hash<auto> options) : AnythingDataType(options) {
    }

    #! returns the type name
    string getName() {
        return "something";
    }

    #! returns True if this type can be assigned from values of the argument type
    bool isAssignableFrom(AbstractDataProviderType t) {
        if (t instanceof SomethingDataType) {
            return True;
        }
        *Type othertype = t.getValueType();
        if (!othertype) {
            return True;
        }
        return !othertype.isEqual(nothingType);
    }

    #! returns True if this type can be assigned from values of the argument type
    bool isAssignableFrom(Type t) {
        return !t.isEqual(nothingType);
    }

    #! returns the base type for the type, if any
    *Type getValueType() {
        # this method intentionally left blank
    }

    #! returns the value if the value can be assigned to the type
    /** @param value the value to assign to the type

        @return the value to be assigned; can be converted by the type

        @throw RUNTIME-TYPE-ERROR value cannot be assigned to the type
    */
    auto acceptsValue(auto value) {
        if (!exists value) {
            throw "RUNTIME-TYPE-ERROR", sprintf("type %y cannot accept \"nothing\"", getName());
        }
        return value;
    }
}

#! Output record iterator for Mapper objects with an input data provider
public class MapperOutputRecordIterator inherits AbstractDataProviderRecordIterator {
    private {
        #! the mapper
        Mapper mapper;

        #! the input data provider
        AbstractDataProviderRecordIterator i;
    }

    #! Creates the object from the arguments
    constructor(Mapper mapper, AbstractDataProvider input, *hash<auto> where_cond, *hash<auto> search_options) {
        self.mapper = mapper;
        i = input.searchRecords(where_cond, search_options);
    }

    #! Moves the current position to the next element; returns @ref False if there are no more elements
    bool next() {
        return i.next();
    }

    #! Returns the mapped value of the current input record
    hash<auto> getValue() {
        return mapper.mapData(i.getValue());
    }

    #! Returns @ref True if the iterator is currently pointing at a valid element, @ref False if not
    bool valid() {
        return i.valid();
    }
}
}
